{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improved state estimation with MPC\n",
    "Copyright (c) 2010-2024, Delft University of Technology. All rights reserved. This file is part of the Tudat. Redistribution and use in source and binary forms, with or without modification, are permitted exclusively under the terms of the Modified BSD license. You should have received a copy of the license with this file. If not, please visit: http://tudat.tudelft.nl/LICENSE.\n",
    "\n",
    "## Objectives\n",
    "This example extends the previous [Initial state estimation with Minor Planet Center Observations](estimation_with_mpc.ipynb). In an attempt to improve the results from the previous example, we introduce and compare the effects of including satellite data, star catalog corrections, observation weighting and more expansive acceleration models. It essential to be familiar with the previous example as many concepts will be reused here without explanation. \n",
    "\n",
    "As in the previous example we will estimate the initial state of [433 Eros](https://en.wikipedia.org/wiki/433_Eros). In addition to observation data from MPC and metadata from SBDB, we now also use ephemeris data from JPL Horizons to retrieve position data for observing space telescopes, additional perturbing bodies and as a method of comparison. This is accomplished using Tudat's HorizonsQuery Interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tudat imports for propagation and estimation\n",
    "from tudatpy.interface import spice\n",
    "from tudatpy import dynamics\n",
    "from tudatpy.dynamics import environment, environment_setup\n",
    "from tudatpy.dynamics import propagation_setup, parameters_setup, simulator\n",
    "from tudatpy import estimation\n",
    "from tudatpy.estimation import observable_models_setup, observable_models, observations_setup, observations, estimation_analysis\n",
    "from tudatpy.constants import GRAVITATIONAL_CONSTANT\n",
    "from tudatpy.astro.frame_conversion import inertial_to_rsw_rotation_matrix\n",
    "from tudatpy.astro.time_representation import DateTime\n",
    "from tudatpy.astro import element_conversion\n",
    "\n",
    "# import MPC, SBDB and Horizons interface\n",
    "from tudatpy.data.mpc import BatchMPC\n",
    "from tudatpy.data.horizons import HorizonsQuery\n",
    "from tudatpy.data.sbdb import SBDBquery\n",
    "\n",
    "\n",
    "# other useful modules\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# SPICE KERNELS\n",
    "spice.load_standard_kernels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the environment and observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the input constants\n",
    "Let's setup some constants that are used throughout the tutorial. The MPC code for Eros is 433. We also set a start and end date for our observations, the number of iterations for our estimation, a timestep for our integrator and a 1 month buffer to avoid interpolation errors in our analysis.\n",
    "\n",
    "We use a spice kernel to get a guess for our initial state and to check our estimation afterwards. The default spice kernel `codes_300ast_20100725.bsp` contains many popular asteroids, however they are not all identified by name (433 Eros is `\"Eros\"` but 16 Psyche is `\"2000016\"` etc.). To ensure this example works dynamically, for any single MPC code as input we use the SDBD to retrieve the name and SPK-ID used for the spice kernel.\n",
    "\n",
    "For our frame origin we use the Solar System Barycentre. The data from MPC is presented in the J2000 reference frame, currently BatchMPC does not support conversion to other reference frames and as such we match it in our environment. \n",
    "\n",
    "For this extended example, a longer observation period of 9 years is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mpc_code = \"433\"\n",
    "\n",
    "observations_start = datetime.datetime(2015, 1, 1)\n",
    "observations_end = datetime.datetime(2024, 1, 1)\n",
    "\n",
    "# number of iterations for our estimation\n",
    "number_of_pod_iterations = 6\n",
    "\n",
    "# timestep of 24 hours for our estimation\n",
    "timestep_global = 24 * 3600\n",
    "\n",
    "# 2 month time buffer used to avoid interpolation errors:\n",
    "time_buffer = 2 * 31 * 86400\n",
    "\n",
    "# define the frame origin and orientation.\n",
    "global_frame_origin = \"SSB\"\n",
    "global_frame_orientation = \"J2000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derived inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPK ID for 433 Eros is: Eros\n"
     ]
    }
   ],
   "source": [
    "target_sbdb = SBDBquery(target_mpc_code)\n",
    "\n",
    "mpc_codes = [target_mpc_code]  # the BatchMPC interface requires a list.\n",
    "target_spkid = target_sbdb.codes_300_spkid  # the ID used by the\n",
    "target_name = target_sbdb.shortname  # the ID used by the\n",
    "\n",
    "print(f\"SPK ID for {target_name} is: {target_spkid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinations and additional body setup\n",
    "There are various ways to change our estimation. We can create a system of setups to compare those various options and to facilitate comparison. Throughout the example, the following options are considered:\n",
    "\n",
    "- [`accel_levels`] Different acceleration settings, for this example, 3 options are created in increasing order of realism\n",
    "\n",
    "    - LVL 1 - Only point-mass gravity for the sun and the 8 mayor planets as well as Schwarzschild relativistic correction for the sun.\n",
    "    - LVL 2 - LVL 1 + point-mass gravity for the mayor moons of Jupiter, Saturn, Earth and Mars.\n",
    "    - LVL 3 - LVL 2 + SHG for the Earth and point-mass gravity for Triton, Titania, Pluto, the mayor Near Earth Asteroids (NEA) and largest Main Body Asteroids (MBA). These additional bodies are retrieved through the JPL Horizons interface.\n",
    "\n",
    "- [`use_sat_data`] Observations by space telescope WISE\n",
    "- [`use_catalog_cor`] Star catalog corrections as described in \"Star catalog position and proper motion corrections in asteroid astrometry II: The Gaia era\" by Eggl et al.\n",
    "- [`use_weighting`] Estimation weights as described in \"Statistical analysis of astrometric errors for the most productive asteroid surveys\" by Veres et al.\n",
    "\n",
    "A function, `perform_estimation`, is be created below which will perform the estimation based on these settings. The settings are described by a series of lists below, with a list of setup names to describe them.\n",
    "\n",
    "The acceleration model is expected to have the most effect on the simulation. For the first round of comparison, only the acceleration models will be changed with the remainder all set to False. Three setups are constructed below. We also define constants to later set up satellite data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_names = [\"LVL1 Accelerations\", \"LVL2 Accelerations\", \"LVL3 Accelerations\"]\n",
    "\n",
    "accel_levels = [1, 2, 3]\n",
    "use_sat_data = [False, False, False]\n",
    "use_catalog_cor = [False, False, False]\n",
    "use_weighting = [False, False, False]\n",
    "\n",
    "satellites_names = [\"WISE\"]\n",
    "satellites_MPC_codes = [\"C51\"] # C51 is the observatory code MPC uses for WISE\n",
    "satellites_Horizons_codes = [\"-163\"]  # -163 is the query ID for WISE in Horizons see explanation below.\n",
    "\n",
    "\n",
    "# Consider trying out different combinations of satellites. \n",
    "# Note that you must change the dates to use TESS as it launched in April 2018\n",
    "# satellites_names = [\"WISE\", \"TESS\"]\n",
    "# satellites_MPC_codes = [\"C51\", \"C57\"]\n",
    "# satellites_Horizons_codes = [\"-163\", \"-95\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LVL3 accelerations, the point-mass gravitational acceleration of Pluto, Triton and Titania are added using JPL Horizons. Horizons only provides an ephemeris, the masses are retrieved and added manually. Note that JPL Horizons has a unique querying scheme in which Pluto is best accessed using the ID 999. The API documentation for the `HorizonsQuery()` class provides an extensive but not exhaustive explanation of these IDs. For now it is sufficient to understand that mayor bodies such as Earth are denoted `399` (3rd mayor body), Asteroids/Minor bodies are denoted with a semicolon like `433;` for Eros (MPC code + ;), and satellites are denote with a minus sign like `-163` for WISE.\n",
    "\n",
    "JPL Horizons will also be used to retrieve the ephemeris for mayor NEA and MBA. Again their masses will be added through other means, in this case we use [SiMDA](https://astro.kretlow.de/simda/), which is an archive of published mass and diameter estimates for minor bodies. \n",
    "\n",
    "All NEAs from the archive are retrieved, as well as all MBA with a mass greater than 1e20 kg. Consider altering this filter to see the effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lvl3_extra_bodies = [\"999\", \"Triton\", \"Titania\"] # here 999 is Pluto in JPL Horizons\n",
    "lvl3_extra_bodies_masses = [1.3025e22, 2.1389e22, 3.4550e21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of additional bodies from SiMDA: 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NUM</th>\n",
       "      <th>DESIGNATION</th>\n",
       "      <th>DIAM</th>\n",
       "      <th>DYN</th>\n",
       "      <th>MASS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>Pallas</td>\n",
       "      <td>520.8</td>\n",
       "      <td>MBA</td>\n",
       "      <td>2.050000e+20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>1036</td>\n",
       "      <td>Ganymed</td>\n",
       "      <td>35.7</td>\n",
       "      <td>NEA</td>\n",
       "      <td>6.580000e+16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>3671</td>\n",
       "      <td>Dionysus</td>\n",
       "      <td>0.9</td>\n",
       "      <td>NEA</td>\n",
       "      <td>8.380000e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>5381</td>\n",
       "      <td>Sekhmet</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NEA</td>\n",
       "      <td>1.040000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>25143</td>\n",
       "      <td>Itokawa</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NEA</td>\n",
       "      <td>3.500000e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>35107</td>\n",
       "      <td>1991 VH</td>\n",
       "      <td>1.1</td>\n",
       "      <td>NEA</td>\n",
       "      <td>1.400000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>65803</td>\n",
       "      <td>Didymos</td>\n",
       "      <td>0.8</td>\n",
       "      <td>NEA</td>\n",
       "      <td>5.240000e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>66063</td>\n",
       "      <td>1998 RO1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NEA</td>\n",
       "      <td>3.600000e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>66391</td>\n",
       "      <td>1999 KW4</td>\n",
       "      <td>1.3</td>\n",
       "      <td>NEA</td>\n",
       "      <td>2.350000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>136617</td>\n",
       "      <td>1994 CC</td>\n",
       "      <td>0.6</td>\n",
       "      <td>NEA</td>\n",
       "      <td>2.590000e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>164121</td>\n",
       "      <td>2003 YT1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>NEA</td>\n",
       "      <td>1.270000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>175706</td>\n",
       "      <td>1996 FG3</td>\n",
       "      <td>1.8</td>\n",
       "      <td>NEA</td>\n",
       "      <td>4.270000e+12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>185851</td>\n",
       "      <td>2000 DP107</td>\n",
       "      <td>1.6</td>\n",
       "      <td>NEA</td>\n",
       "      <td>4.600000e+11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>276049</td>\n",
       "      <td>2002 CE26</td>\n",
       "      <td>3.5</td>\n",
       "      <td>NEA</td>\n",
       "      <td>1.950000e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>311066</td>\n",
       "      <td>2004 DC</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NEA</td>\n",
       "      <td>3.570000e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>494658</td>\n",
       "      <td>2000 UG11</td>\n",
       "      <td>0.3</td>\n",
       "      <td>NEA</td>\n",
       "      <td>9.350000e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        NUM DESIGNATION   DIAM  DYN          MASS\n",
       "19        2      Pallas  520.8  MBA  2.050000e+20\n",
       "384    1036     Ganymed   35.7  NEA  6.580000e+16\n",
       "395    3671    Dionysus    0.9  NEA  8.380000e+11\n",
       "398    5381     Sekhmet    1.0  NEA  1.040000e+12\n",
       "399   25143     Itokawa    0.3  NEA  3.500000e+10\n",
       "401   35107     1991 VH    1.1  NEA  1.400000e+12\n",
       "407   65803     Didymos    0.8  NEA  5.240000e+11\n",
       "408   66063    1998 RO1    0.7  NEA  3.600000e+11\n",
       "409   66391    1999 KW4    1.3  NEA  2.350000e+12\n",
       "418  136617     1994 CC    0.6  NEA  2.590000e+11\n",
       "421  164121    2003 YT1    1.1  NEA  1.270000e+12\n",
       "422  175706    1996 FG3    1.8  NEA  4.270000e+12\n",
       "423  185851  2000 DP107    1.6  NEA  4.600000e+11\n",
       "424  276049   2002 CE26    3.5  NEA  1.950000e+13\n",
       "425  311066     2004 DC    0.3  NEA  3.570000e+10\n",
       "426  494658   2000 UG11    0.3  NEA  9.350000e+09"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"SiMDA_240512.csv\"\n",
    "\n",
    "min_asteroid_mass = 1e20 # kg\n",
    "target_int = int(target_mpc_code)\n",
    "\n",
    "simda = (\n",
    "    pd.read_csv(file)\n",
    "    .iloc[18:] # the first 18 rows contain comets, which are omitted\n",
    "    .assign(NUM=lambda x: np.int32(x.NUM))\n",
    "    .query(\"DYN == 'NEA' | (DYN == 'MBA' & MASS > @min_asteroid_mass)\") # filter relevant bodies\n",
    "    .query(\"NUM != @target_int\") # remove 433 Eros, which is also a NEA\n",
    "    .query(\"NUM != [1, 4]\") # remove Ceres and Vesta which are retrieved through spice kernels\n",
    "    .loc[:, [\"NUM\", \"DESIGNATION\", \"DIAM\", \"DYN\", \"MASS\"]]\n",
    ")\n",
    "\n",
    "lvl3_asteroids = simda.NUM.to_list()\n",
    "lvl3_asteroids_masses = simda.MASS.to_list()\n",
    "\n",
    "print(f\"Number of additional bodies from SiMDA: {len(simda)}\")\n",
    "\n",
    "simda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the observations\n",
    "As in the previous example, we retrieve observation data using BatchMPC and the initial position using spice. \n",
    "\n",
    "In the previous example, a random offset was added to the position and velocity of the initial position. To enable better comparison, this random offset has been omitted for this example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of space telescopes in batch:\n",
      "     Code                                    Name   count\n",
      "274   275  Non-geocentric Occultation Observation     3.0\n",
      "1232  C51                                    WISE   155.0\n",
      "1238  C57                                    TESS  1620.0\n",
      "1240  C59                              Yangwang-1     2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/tudat-bundle/lib/python3.11/site-packages/tudatpy/data/mpc/mpc.py:863: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['433' '433' '433' ... '433' '433' '433']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  obs.loc[:, \"number\"] = obs.number.astype(str)\n"
     ]
    }
   ],
   "source": [
    "batch = BatchMPC()\n",
    "batch.get_observations(mpc_codes)\n",
    "batch.filter(\n",
    "    epoch_start=observations_start,\n",
    "    epoch_end=observations_end,\n",
    ")\n",
    "\n",
    "# Retrieve the first and final observation epochs and add the buffer\n",
    "epoch_start_nobuffer = batch.epoch_start\n",
    "epoch_end_nobuffer = batch.epoch_end\n",
    "\n",
    "# This samples the cartesian state at 500 points over the observation time:\n",
    "times_get_eph = np.linspace(epoch_start_nobuffer, epoch_end_nobuffer, 500)\n",
    "\n",
    "epoch_start_buffer = epoch_start_nobuffer - time_buffer\n",
    "epoch_end_buffer = epoch_end_nobuffer + time_buffer\n",
    "\n",
    "initial_guess = spice.get_body_cartesian_state_at_epoch(\n",
    "    target_spkid,\n",
    "    global_frame_origin,\n",
    "    global_frame_orientation,\n",
    "    \"NONE\",\n",
    "    epoch_start_buffer,\n",
    ")\n",
    "\n",
    "print(\"Summary of space telescopes in batch:\")\n",
    "print(batch.observatories_table(only_space_telescopes=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving satellite and astroid ephemerides from JPL Horizons\n",
    "Below we retrieve and store satellite and asteroid ephemerides from JPL Horizons. The HorizonsQuery class included with Tudat's data module provides quick access to ephemeris data for many objects in our solar system. For this example, we provide a start and end date based on the buffered first and last observation dates. We then request the state of the target object at every timestep, centered at our global frame origin and with our global frame orientation. The `.create_ephemeris_tabulated()` method then creates an ephemeris in Tudat format which is then stored for later use.\n",
    "\n",
    "Tudat uses interpolation to generate an ephemeris model from the tabulated positions and velocities retrieved from JPL Horizons. To speed up the process, we increase the timestep to 5x24 hours. For the satellites we keep the original timestep as their fast dynamics (Geocentric orbits), would yield inaccurate interpolations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep_horizons = timestep_global * 5\n",
    "\n",
    "# Ephemeris for satellite(s)\n",
    "sat_ephemeris = {}\n",
    "for code, name in zip(satellites_Horizons_codes, satellites_names):\n",
    "    query = HorizonsQuery(\n",
    "        query_id=code,\n",
    "        location=f\"@{global_frame_origin}\",\n",
    "        epoch_start=epoch_start_buffer,\n",
    "        epoch_end=epoch_end_buffer,\n",
    "        epoch_step=f\"{int(timestep_global/60)}m\", # Horizons does not permit a stepsize in seconds\n",
    "        extended_query=True, # extended query allows for more data to be retrieved.\n",
    "    )\n",
    "\n",
    "    sat_ephemeris[name] = query.create_ephemeris_tabulated(\n",
    "        frame_origin=global_frame_origin,\n",
    "        frame_orientation=global_frame_orientation,\n",
    "    )\n",
    "\n",
    "# Ephemeris for asteroids\n",
    "ast_ephemeris = {}\n",
    "for code in lvl3_asteroids:\n",
    "    query = HorizonsQuery(\n",
    "        query_id=f\"{code};\",\n",
    "        location=f\"@{global_frame_origin}\",\n",
    "        epoch_start=epoch_start_buffer - 12 * 31 * 86400,\n",
    "        epoch_end=epoch_end_buffer + 12 * 31 * 86400,\n",
    "        epoch_step=f\"{int(timestep_horizons/60)}m\",\n",
    "        extended_query=True,\n",
    "    )\n",
    "\n",
    "    ast_ephemeris[code] = query.create_ephemeris_tabulated(\n",
    "        frame_origin=global_frame_origin,\n",
    "        frame_orientation=global_frame_orientation,\n",
    "    )\n",
    "\n",
    "# Ephemeris for Pluto, Triton and Titania\n",
    "other_ephemeris = {}\n",
    "for code in lvl3_extra_bodies:\n",
    "    query = HorizonsQuery(\n",
    "        query_id=f\"{code}\",\n",
    "        location=f\"@{global_frame_origin}\",\n",
    "        epoch_start=epoch_start_buffer - 12 * 31 * 86400,\n",
    "        epoch_end=epoch_end_buffer + 12 * 31 * 86400,\n",
    "        epoch_step=f\"{int(timestep_horizons/60)}m\",\n",
    "        extended_query=True,\n",
    "    )\n",
    "\n",
    "    other_ephemeris[code] = query.create_ephemeris_tabulated(\n",
    "        frame_origin=global_frame_origin,\n",
    "        frame_orientation=global_frame_orientation,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment\n",
    "As in the previous example, we use `get_default_body_settings()` to retrieve body settings for the main bodies from SPICE. Additional bodies are added using the `add_empty_settings()` method, which then gets ammended with our ephemerides retrieved previously. For the additional perturbing bodies, we add a central point mass gravity field, which takes a gravitational parameter, here calculated from the masses obtained from simda and elsewhere. We use the same body settings for every setup in this example, altering the effects by means of differing acceleration settings.\n",
    "\n",
    "We again also retrieve the bodies to propagate and central bodies required for our integrator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of bodies to be retrieved through SPICE.\n",
    "bodies_SPICE = [\n",
    "    \"Sun\",\n",
    "    \"Mercury\",\n",
    "    \"Venus\",\n",
    "    \"Earth\",\n",
    "    \"Moon\",\n",
    "    \"Mars\",\n",
    "    \"Phobos\",\n",
    "    \"Deimos\",\n",
    "    \"Ceres\",\n",
    "    \"Vesta\",\n",
    "    \"Jupiter\",\n",
    "    \"Io\",\n",
    "    \"Europa\",\n",
    "    \"Ganymede\",\n",
    "    \"Callisto\",\n",
    "    \"Saturn\",\n",
    "    \"Titan\",\n",
    "    \"Rhea\",\n",
    "    \"Iapetus\",\n",
    "    \"Dione\",\n",
    "    \"Tethys\",\n",
    "    \"Enceladus\",\n",
    "    \"Mimas\",\n",
    "    \"Uranus\",\n",
    "    \"Neptune\",\n",
    "]\n",
    "\n",
    "\n",
    "# Create system of bodies through SPICE\n",
    "body_settings = environment_setup.get_default_body_settings(\n",
    "    bodies_SPICE, global_frame_origin, global_frame_orientation\n",
    ")\n",
    "\n",
    "# Add satellite(s) and their ephemerides to body settings\n",
    "for name in satellites_names:\n",
    "    body_settings.add_empty_settings(name)\n",
    "    body_settings.get(name).ephemeris_settings = sat_ephemeris[name]\n",
    "\n",
    "\n",
    "# Add asteroids, their ephemerides and gravity field to body settings\n",
    "for asteroid_code, asteroid_mass in zip(lvl3_asteroids, lvl3_asteroids_masses):\n",
    "    body_settings.add_empty_settings(str(asteroid_code))\n",
    "    body_settings.get(str(asteroid_code)).ephemeris_settings = ast_ephemeris[asteroid_code]\n",
    "    body_settings.get(str(asteroid_code)).gravity_field_settings = (\n",
    "        environment_setup.gravity_field.central(asteroid_mass * GRAVITATIONAL_CONSTANT)\n",
    "    )\n",
    "\n",
    "# Add Pluto, Triton and Titania and their ephemerides and gravity field to body settings\n",
    "for other_code, other_mass in zip(lvl3_extra_bodies, lvl3_extra_bodies_masses):\n",
    "    body_settings.add_empty_settings(str(other_code))\n",
    "    body_settings.get((other_code)).ephemeris_settings = other_ephemeris[other_code]\n",
    "    body_settings.get((other_code)).gravity_field_settings = (\n",
    "        environment_setup.gravity_field.central(asteroid_mass * GRAVITATIONAL_CONSTANT)\n",
    "    )\n",
    "\n",
    "bodies = environment_setup.create_system_of_bodies(body_settings)\n",
    "\n",
    "# Retrieve Eros' body name from BatchMPC and set its centre to enable its propapgation\n",
    "bodies_to_propagate = batch.MPC_objects\n",
    "central_bodies = [global_frame_origin] * len(batch.MPC_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the acceleration settings\n",
    "Differing acceleration settings will allow us to see how additional perturbations affect our estimation. As mentioned before the following acceleration sets are used:\n",
    "\n",
    "- LVL 1 - Only point-mass gravity for the sun and the 8 mayor planets as well as Schwarzschild relativistic correction for the sun.\n",
    "- LVL 2 - LVL 1 + point-mass gravity for the mayor moons of Jupiter, Saturn, Earth and Mars.\n",
    "- LVL 3 - LVL 2 + SHG for the Earth and point-mass gravity for Triton, Titania, Pluto, the mayor Near Earth Asteroids (NEA) and largest Main Body Asteroids (MBA). \n",
    "\n",
    "Note that LVL 1 represents the same acceleration settings used for the first example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LVL 1, from the basic example\n",
    "accelerations_1 = {\n",
    "    \"Sun\": [\n",
    "        propagation_setup.acceleration.point_mass_gravity(),\n",
    "        propagation_setup.acceleration.relativistic_correction(use_schwarzschild=True),\n",
    "    ],\n",
    "    \"Mercury\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Venus\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Earth\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Moon\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Mars\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Jupiter\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Saturn\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Uranus\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Neptune\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "}\n",
    "\n",
    "# LVL 2\n",
    "accelerations_2 = {\n",
    "    \"Sun\": [\n",
    "        propagation_setup.acceleration.point_mass_gravity(),\n",
    "        propagation_setup.acceleration.relativistic_correction(use_schwarzschild=True),\n",
    "    ],\n",
    "    \"Mercury\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Venus\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "\n",
    "    \"Earth\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Moon\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "\n",
    "    \"Mars\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "\n",
    "    \"Jupiter\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Io\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Europa\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Ganymede\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Callisto\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "\n",
    "    \"Saturn\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Titan\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Rhea\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Iapetus\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Dione\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \n",
    "    \"Uranus\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "\n",
    "    \"Neptune\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "}\n",
    "\n",
    "# LVL 3\n",
    "accelerations_3 = {\n",
    "    \"Sun\": [\n",
    "        propagation_setup.acceleration.point_mass_gravity(),\n",
    "        propagation_setup.acceleration.relativistic_correction(use_schwarzschild=True),\n",
    "    ],\n",
    "    \"Mercury\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Venus\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Earth\": [\n",
    "        propagation_setup.acceleration.spherical_harmonic_gravity(2, 2),\n",
    "    ],\n",
    "    \"Moon\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \n",
    "    \"Mars\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Phobos\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Deimos\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \n",
    "    \"Ceres\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Vesta\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "\n",
    "    \"Jupiter\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "\n",
    "    \"Io\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Europa\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Ganymede\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Callisto\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \n",
    "    \"Saturn\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Titan\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Rhea\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Iapetus\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Dione\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Tethys\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Enceladus\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Mimas\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "\n",
    "    \"Uranus\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Neptune\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "}\n",
    "\n",
    "# For each asteroid + Pluto, Titania and Triton we create a point mass gravity.\n",
    "asteroid_accelerations = {str(num):[propagation_setup.acceleration.point_mass_gravity()] for num in lvl3_asteroids}\n",
    "other_accelerations = {str(num):[propagation_setup.acceleration.point_mass_gravity()] for num in lvl3_extra_bodies}\n",
    "\n",
    "# we combine the accelerations to achieve the final LVL 3 set\n",
    "accelerations_3 = (accelerations_3 | asteroid_accelerations) | other_accelerations\n",
    "\n",
    "# Dictionary with the three acceleration setting options\n",
    "acceleration_sets = {1: accelerations_1, 2: accelerations_2, 3:accelerations_3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalising the propagation setup\n",
    "We use the same fixed timestep RKF-7(8) integrator as before, with the buffered start and termination times and global timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "runge_kutta_variable_step_size(): incompatible function arguments. The following argument types are supported:\n    1. (initial_time_step: tudatpy.kernel.astro.time_representation.Time, coefficient_set: tudatpy.kernel.dynamics.propagation_setup.integrator.CoefficientSets, minimum_step_size: tudatpy.kernel.astro.time_representation.Time, maximum_step_size: tudatpy.kernel.astro.time_representation.Time, relative_error_tolerance: typing.SupportsFloat, absolute_error_tolerance: typing.SupportsFloat, assess_termination_on_minor_steps: bool = False, safety_factor: tudatpy.kernel.astro.time_representation.Time = 0.8, maximum_factor_increase: tudatpy.kernel.astro.time_representation.Time = 4.0, minimum_factor_increase: tudatpy.kernel.astro.time_representation.Time = 0.1, throw_exception_if_minimum_step_exceeded: bool = True) -> tudatpy.kernel.dynamics.propagation_setup.integrator.IntegratorSettings\n    2. (initial_time: tudatpy.kernel.astro.time_representation.Time, initial_time_step: tudatpy.kernel.astro.time_representation.Time, coefficient_set: tudatpy.kernel.dynamics.propagation_setup.integrator.CoefficientSets, minimum_step_size: tudatpy.kernel.astro.time_representation.Time, maximum_step_size: tudatpy.kernel.astro.time_representation.Time, relative_error_tolerance: typing.SupportsFloat, absolute_error_tolerance: typing.SupportsFloat, assess_termination_on_minor_steps: bool = False, safety_factor: tudatpy.kernel.astro.time_representation.Time = 0.8, maximum_factor_increase: tudatpy.kernel.astro.time_representation.Time = 4.0, minimum_factor_increase: tudatpy.kernel.astro.time_representation.Time = 0.1, throw_exception_if_minimum_step_exceeded: bool = True) -> tudatpy.kernel.dynamics.propagation_setup.integrator.IntegratorSettings\n\nInvoked with: 505664109.4335437, 86400, <CoefficientSets.rkf_78: 14>, 86400, 86400, 1.0, 1.0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create numerical integrator settings\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m integrator_settings = \u001b[43mpropagation_setup\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintegrator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunge_kutta_variable_step_size\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepoch_start_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestep_global\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpropagation_setup\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintegrator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCoefficientSets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrkf_78\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestep_global\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimestep_global\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Terminate at the time of oldest observation\u001b[39;00m\n\u001b[32m     13\u001b[39m termination_condition = propagation_setup.propagator.time_termination(epoch_end_buffer)\n",
      "\u001b[31mTypeError\u001b[39m: runge_kutta_variable_step_size(): incompatible function arguments. The following argument types are supported:\n    1. (initial_time_step: tudatpy.kernel.astro.time_representation.Time, coefficient_set: tudatpy.kernel.dynamics.propagation_setup.integrator.CoefficientSets, minimum_step_size: tudatpy.kernel.astro.time_representation.Time, maximum_step_size: tudatpy.kernel.astro.time_representation.Time, relative_error_tolerance: typing.SupportsFloat, absolute_error_tolerance: typing.SupportsFloat, assess_termination_on_minor_steps: bool = False, safety_factor: tudatpy.kernel.astro.time_representation.Time = 0.8, maximum_factor_increase: tudatpy.kernel.astro.time_representation.Time = 4.0, minimum_factor_increase: tudatpy.kernel.astro.time_representation.Time = 0.1, throw_exception_if_minimum_step_exceeded: bool = True) -> tudatpy.kernel.dynamics.propagation_setup.integrator.IntegratorSettings\n    2. (initial_time: tudatpy.kernel.astro.time_representation.Time, initial_time_step: tudatpy.kernel.astro.time_representation.Time, coefficient_set: tudatpy.kernel.dynamics.propagation_setup.integrator.CoefficientSets, minimum_step_size: tudatpy.kernel.astro.time_representation.Time, maximum_step_size: tudatpy.kernel.astro.time_representation.Time, relative_error_tolerance: typing.SupportsFloat, absolute_error_tolerance: typing.SupportsFloat, assess_termination_on_minor_steps: bool = False, safety_factor: tudatpy.kernel.astro.time_representation.Time = 0.8, maximum_factor_increase: tudatpy.kernel.astro.time_representation.Time = 4.0, minimum_factor_increase: tudatpy.kernel.astro.time_representation.Time = 0.1, throw_exception_if_minimum_step_exceeded: bool = True) -> tudatpy.kernel.dynamics.propagation_setup.integrator.IntegratorSettings\n\nInvoked with: 505664109.4335437, 86400, <CoefficientSets.rkf_78: 14>, 86400, 86400, 1.0, 1.0"
     ]
    }
   ],
   "source": [
    "# Create numerical integrator settings\n",
    "integrator_settings = propagation_setup.integrator.runge_kutta_variable_step_size(\n",
    "    epoch_start_buffer,\n",
    "    timestep_global,\n",
    "    propagation_setup.integrator.CoefficientSets.rkf_78,\n",
    "    timestep_global,\n",
    "    timestep_global,\n",
    "    1.0,\n",
    "    1.0,\n",
    ")\n",
    "\n",
    "# Terminate at the time of oldest observation\n",
    "termination_condition = propagation_setup.propagator.time_termination(epoch_end_buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation and plotting functions\n",
    "To enable standardised comparison of the different setups, we create estimation and plotting functions for our estimations. The estimation itself largely follows the same steps as the previous example, with the exception of the added satellite configuration and the enabling of the weights and the star catalog corrections. The following four functions are made:\n",
    "\n",
    "- [`perform_estimation`] takes a set of options defined by us to perform an estimation, returning the `estimator`, `pod_output`, `batch` and `observation_collection` for subsequent analysis.\n",
    "- [`plot_residuals`] plots the obtained residuals for a collection of setups\n",
    "- [`plot_cartesian`] plots the carthesian error with respect to SPICE and JPL Horizons for a collection of setups.\n",
    "- [`plot_cartesian_single`] plots a more detailed verion of the above for a single setup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_estimation(\n",
    "    bodies,\n",
    "    acceleration_level:int,\n",
    "    use_satellite_data: bool,\n",
    "    apply_star_catalog_debias: bool,\n",
    "    apply_weighting_scheme: bool,\n",
    "):\n",
    "    # The satellites are present in the integration of all setups, \n",
    "    # the included satellitess parameter in to_tudat() dictates whether a satellite's observations are used.\n",
    "    if use_satellite_data:\n",
    "        included_satellites = {\n",
    "            mpc: name for mpc, name in zip(satellites_MPC_codes, satellites_names)\n",
    "        }\n",
    "    else:\n",
    "        included_satellites = None\n",
    "\n",
    "    # As in the first example, the observation collection is created with BatchMPC.to_tudat()\n",
    "    # This time, the star catalog biases and weights are enabled,\n",
    "    # the included_satellites parameter ensures satellite observations are included.\n",
    "    # internally, to_tudat() links a space telescope's observatory code to the spacecraft's dynamics.\n",
    "    batch_temp = batch.copy()\n",
    "    observation_collection = batch_temp.to_tudat(\n",
    "        bodies=bodies,\n",
    "        included_satellites=included_satellites,\n",
    "        apply_star_catalog_debias=apply_star_catalog_debias,\n",
    "        apply_weights_VFCC17=apply_weighting_scheme,\n",
    "    )\n",
    "\n",
    "    # Set up the accelerations settings for each body, in this case only Eros\n",
    "    acceleration_settings = {}\n",
    "    for body in bodies_to_propagate:\n",
    "        acceleration_settings[str(body)] = acceleration_sets[acceleration_level]\n",
    "\n",
    "    # Create the acceleration models.\n",
    "    acceleration_models = propagation_setup.create_acceleration_models(\n",
    "        bodies, acceleration_settings, bodies_to_propagate, central_bodies\n",
    "    )\n",
    "\n",
    "    # Set create angular_position settings for each link in the list.\n",
    "    observation_settings_list = list()\n",
    "    link_list = list(\n",
    "        observation_collection.get_link_definitions_for_observables(\n",
    "            observable_type=observable_models_setup.model_settings.angular_position_type\n",
    "        )\n",
    "    )\n",
    "    for link in link_list:\n",
    "        observation_settings_list.append(\n",
    "            observable_models_setup.model_settings.angular_position(link, bias_settings=None)\n",
    "        )\n",
    "\n",
    "    # Create propagation settings\n",
    "    propagator_settings = propagation_setup.propagator.translational(\n",
    "        central_bodies=central_bodies,\n",
    "        acceleration_models=acceleration_models,\n",
    "        bodies_to_integrate=bodies_to_propagate,\n",
    "        initial_states=initial_guess,\n",
    "        initial_time=epoch_start_buffer,\n",
    "        integrator_settings=integrator_settings,\n",
    "        termination_settings=termination_condition,\n",
    "    )\n",
    "\n",
    "    # Setup parameters settings to propagate the state transition matrix\n",
    "    parameter_settings = parameters_setup.initial_states(\n",
    "        propagator_settings, bodies\n",
    "    )\n",
    "\n",
    "    # Create the parameters that will be estimated\n",
    "    parameters_to_estimate = parameters_setup.create_parameter_set(\n",
    "        parameter_settings, bodies, propagator_settings\n",
    "    )\n",
    "\n",
    "    # Set up the estimator\n",
    "    estimator = estimation_analysis.Estimator(\n",
    "        bodies=bodies,\n",
    "        estimated_parameters=parameters_to_estimate,\n",
    "        observation_settings=observation_settings_list,\n",
    "        propagator_settings=propagator_settings,\n",
    "        integrate_on_creation=True,\n",
    "    )\n",
    "\n",
    "    # provide the observation collection as input, and limit number of iterations for estimation.\n",
    "    pod_input = estimation_analysis.EstimationInput(\n",
    "        observations_and_times=observation_collection,\n",
    "        convergence_checker=estimation_analysis.estimation_convergence_checker(\n",
    "            maximum_iterations=number_of_pod_iterations,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # to_tudat() applies weights to a set of observations between an observatory and the target.\n",
    "    # the method below tells tudat to use the weights applied to these sets.\n",
    "    # This step is required when setting weights through the BatchMPC class.\n",
    "    if apply_weighting_scheme:\n",
    "        pod_input.set_weights_from_observation_collection()\n",
    "\n",
    "    # Set methodological options\n",
    "    pod_input.define_estimation_settings(reintegrate_variational_equations=True)\n",
    "\n",
    "    # Perform the estimation\n",
    "    pod_output = estimator.perform_estimation(pod_input)\n",
    "\n",
    "    # we store the following outputs for plotting and analysis.\n",
    "    return pod_output, batch_temp, observation_collection, estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(\n",
    "    setup_names: list,\n",
    "    pod_output_set: list,\n",
    "    observation_collection_set: list,\n",
    "):\n",
    "    number_of_columns = len(pod_output_set)\n",
    "\n",
    "    iters_to_use = list(range(0, number_of_pod_iterations))\n",
    "    number_of_rows = len(iters_to_use)\n",
    "\n",
    "    fig, axs = plt.subplots(\n",
    "        number_of_rows,\n",
    "        number_of_columns,\n",
    "        figsize=(number_of_columns * 4.0, 3.5 * number_of_rows),\n",
    "        sharex=True,\n",
    "        sharey=False,\n",
    "    )\n",
    "\n",
    "    if len(axs.shape) == 1:\n",
    "        axs = np.reshape(axs, (len(axs), 1))\n",
    "\n",
    "    for setup_idx, (p_out, obs_col, setup_name) in enumerate(\n",
    "        zip(pod_output_set, observation_collection_set, setup_names)\n",
    "    ):\n",
    "        residual_history = p_out.residual_history\n",
    "\n",
    "        # We cheat a little to get an approximate year out of our times (which are in seconds since J2000)\n",
    "        residual_times = np.array(obs_col.concatenated_times) / (86400 * 365.25) + 2000\n",
    "\n",
    "        # plot the residuals, split between RA and DEC types\n",
    "        for i in range(number_of_rows):\n",
    "            axs[i, setup_idx].grid()\n",
    "\n",
    "            axs[i, setup_idx].scatter(\n",
    "                residual_times[::2],\n",
    "                residual_history[\n",
    "                    ::2,\n",
    "                    i,\n",
    "                ],\n",
    "                marker=\"+\",\n",
    "                s=30,\n",
    "                label=\"Right Ascension\",\n",
    "            )\n",
    "            axs[i, setup_idx].scatter(\n",
    "                residual_times[1::2],\n",
    "                residual_history[\n",
    "                    1::2,\n",
    "                    i,\n",
    "                ],\n",
    "                marker=\"+\",\n",
    "                s=30,\n",
    "                label=\"Declination\",\n",
    "            )\n",
    "\n",
    "            if i == 0:\n",
    "                axs[i, setup_idx].set_title(\n",
    "                    f\"Setup: {setup_name}\\n\" + \"Iteration \" + str(i + 1)\n",
    "                )\n",
    "            else:\n",
    "                axs[i, setup_idx].set_title(\"Iteration \" + str(i + 1))\n",
    "\n",
    "            if setup_idx == 0:\n",
    "                axs[i, setup_idx].set_ylabel(\"Observation Residual [rad]\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # add the year label for the x-axis\n",
    "    for col in range(number_of_columns):\n",
    "        axs[int(number_of_rows - 1), col].set_xlabel(\"Year\")\n",
    "\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cartesian(\n",
    "    state_estimates_set: list,\n",
    "    setup_names: list,\n",
    "    observation_collection_set: list,\n",
    "):\n",
    "    # lets get ranges for all gaps in observations larger than 6 months:\n",
    "    residual_times = (\n",
    "        np.array(observation_collection_set[0].concatenated_times) / (86400 * 365.25)\n",
    "        + 2000\n",
    "    )\n",
    "    gap_in_months = 6\n",
    "    gaps = np.abs(np.diff(sorted(residual_times)))\n",
    "    num_gaps = (\n",
    "        gaps > (gap_in_months / 12)\n",
    "    ).sum()  # counts the number of gaps larger than 0.5 years\n",
    "    indices_of_largest_gaps = np.argsort(gaps)[-num_gaps:]\n",
    "\n",
    "    # (start, end) for each of the gaps\n",
    "    gap_ranges = [\n",
    "        (sorted(residual_times)[idx - 1], sorted(residual_times)[idx + 1])\n",
    "        for idx in indices_of_largest_gaps\n",
    "    ]\n",
    "\n",
    "    fig, axs = plt.subplots(3, 2, figsize=(12, 15))\n",
    "    # retrieve the states for a list of times in:\n",
    "    # SPICE\n",
    "    spice_states = []\n",
    "    for timee in times_get_eph:\n",
    "        # from spice\n",
    "        state_spice = spice.get_body_cartesian_state_at_epoch(\n",
    "            target_spkid, central_bodies[0], global_frame_orientation, \"NONE\", timee\n",
    "        )\n",
    "        spice_states.append(state_spice)\n",
    "\n",
    "    # Horizons\n",
    "    horizons_query = HorizonsQuery(\n",
    "        query_id=f\"{target_mpc_code};\",\n",
    "        location=f\"500@{global_frame_origin}\",\n",
    "        epoch_list=list(times_get_eph),\n",
    "        extended_query=True,\n",
    "    )\n",
    "    horizons_states = horizons_query.cartesian(\n",
    "        frame_orientation=global_frame_orientation\n",
    "    )[:, 1:]\n",
    "\n",
    "    times_plot = times_get_eph / (86400 * 365.25) + 2000  # approximate for plot ticks\n",
    "    # Get the errors per cartesian component and plot.\n",
    "    for state_estt, setup_name in zip(state_estimates_set, setup_names):\n",
    "        # Error in kilometers\n",
    "        error_spice = (np.array(spice_states) - np.array(state_estt)) / 1000\n",
    "        error_jpl = (horizons_states - np.array(state_estt)) / 1000\n",
    "\n",
    "        # plot\n",
    "        axs[0, 0].plot(times_plot, error_spice[:, 0], label=setup_name)\n",
    "        axs[1, 0].plot(times_plot, error_spice[:, 1], label=setup_name)\n",
    "        axs[2, 0].plot(times_plot, error_spice[:, 2], label=setup_name)\n",
    "\n",
    "        axs[0, 1].plot(times_plot, error_jpl[:, 0], label=setup_name)\n",
    "        axs[1, 1].plot(times_plot, error_jpl[:, 1], label=setup_name)\n",
    "        axs[2, 1].plot(times_plot, error_jpl[:, 2], label=setup_name)\n",
    "\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        # show areas where there are no observations:\n",
    "        for i, gap in enumerate(gap_ranges):\n",
    "            ax.axvspan(\n",
    "                xmin=gap[0],\n",
    "                xmax=gap[1],\n",
    "                color=\"red\" if idx % 2 == 0 else \"blue\",\n",
    "                alpha=0.1,\n",
    "                label=\"Large gap in observations\" if i == 0 else None,\n",
    "            )\n",
    "        ax.grid()\n",
    "\n",
    "    axs[0, 0].legend(ncol=1)\n",
    "    axs[0, 0].set_ylabel(\"X Cartesian Error [km]\")\n",
    "    axs[1, 0].set_ylabel(\"Y Cartesian Error [km]\")\n",
    "    axs[2, 0].set_ylabel(\"Z Cartesian Error [km]\")\n",
    "    axs[2, 0].set_xlabel(\"Year\")\n",
    "\n",
    "    axs[0, 0].set_title(f\"Error vs SPICE over time for {target_name}\")\n",
    "    axs[0, 1].set_title(f\"Error vs JPL HORIZONS over time for {target_name}\")\n",
    "    fig.set_tight_layout(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cartesian_single(\n",
    "    state_estimate,\n",
    "    setup_name,\n",
    "    observation_collection,\n",
    "    in_RSW_frame=False,\n",
    "):\n",
    "    # lets get ranges for all gaps in observations larger than 6 months:\n",
    "    gap_in_months = 6\n",
    "    residual_times = (\n",
    "        np.array(observation_collection.concatenated_times) / (86400 * 365.25) + 2000\n",
    "    )\n",
    "    gaps = np.abs(np.diff(sorted(residual_times)))\n",
    "    num_gaps = (\n",
    "        gaps > (gap_in_months / 12)\n",
    "    ).sum()  # counts the number of gaps larger than 0.5 years\n",
    "    indices_of_largest_gaps = np.argsort(gaps)[-num_gaps:]\n",
    "    # (start, end) for each of the gaps\n",
    "    gap_ranges = [\n",
    "        (sorted(residual_times)[idx - 1], sorted(residual_times)[idx + 1])\n",
    "        for idx in indices_of_largest_gaps\n",
    "    ]\n",
    "\n",
    "    # retrieve the states for a list of times in SPICE and Horizons:\n",
    "    spice_states = []\n",
    "    for timee in times_get_eph:\n",
    "        # from spice\n",
    "        state_spice = spice.get_body_cartesian_state_at_epoch(\n",
    "            target_spkid, central_bodies[0], global_frame_orientation, \"NONE\", timee\n",
    "        )\n",
    "        spice_states.append(state_spice)\n",
    "\n",
    "    horizons_query = HorizonsQuery(\n",
    "        query_id=f\"{target_mpc_code};\",\n",
    "        location=f\"500@{global_frame_origin}\",\n",
    "        epoch_list=list(times_get_eph),\n",
    "        extended_query=True,\n",
    "    )\n",
    "    horizons_states = horizons_query.cartesian(\n",
    "        frame_orientation=global_frame_orientation\n",
    "    )[:, 1:]\n",
    "\n",
    "    if in_RSW_frame:\n",
    "        # convert to RSW frame\n",
    "        error_spice = [\n",
    "            (inertial_to_rsw_rotation_matrix(state_E) @ (state_E[0:3] - state_S[:3]))\n",
    "            / 1000\n",
    "            for (state_S, state_E) in zip(spice_states, state_estimate)\n",
    "        ]\n",
    "        error_jpl = [\n",
    "            (inertial_to_rsw_rotation_matrix(state_E) @ (state_E[0:3] - state_H[:3]))\n",
    "            / 1000\n",
    "            for (state_H, state_E) in zip(horizons_states, state_estimate)\n",
    "        ]\n",
    "        error_spice = np.array(error_spice)\n",
    "        error_jpl = np.array(error_jpl)\n",
    "        frame_name = \"RSW\"\n",
    "    else:\n",
    "        error_spice = (np.array(spice_states) - np.array(state_estimate)) / 1000\n",
    "        error_jpl = (horizons_states - np.array(state_estimate)) / 1000\n",
    "        frame_name = \"Cartesian\"\n",
    "\n",
    "    # plot\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12, 9))\n",
    "    times_plot = times_get_eph / (86400 * 365.25) + 2000  # approximate for plot ticks\n",
    "    axs[0].plot(times_plot, error_spice[:, 0], label=\"Radial\" if in_RSW_frame else \"X\")\n",
    "    axs[0].plot(times_plot, error_spice[:, 1], label=\"Along-Track\" if in_RSW_frame else \"Y\")\n",
    "    axs[0].plot(times_plot, error_spice[:, 2], label=\"Cross-Track\" if in_RSW_frame else \"Z\")\n",
    "    axs[0].plot(\n",
    "        times_plot,\n",
    "        np.linalg.norm(error_spice[:, :3], axis=1),\n",
    "        label=\"magnitude\",\n",
    "        linestyle=\"--\",\n",
    "        color=\"k\",\n",
    "    )\n",
    "\n",
    "    axs[1].plot(times_plot, error_jpl[:, 0])\n",
    "    axs[1].plot(times_plot, error_jpl[:, 1])\n",
    "    axs[1].plot(times_plot, error_jpl[:, 2])\n",
    "    axs[1].plot(\n",
    "        times_plot, np.linalg.norm(error_jpl[:, :3], axis=1), linestyle=\"--\", color=\"k\"\n",
    "    )\n",
    "\n",
    "    for idx, ax in enumerate(axs.flatten()):\n",
    "        # show areas where there are no observations:\n",
    "        for i, gap in enumerate(gap_ranges):\n",
    "            if in_RSW_frame:\n",
    "                color = \"orange\" if idx % 2 == 0 else \"purple\"\n",
    "            else:\n",
    "                color = \"red\" if idx % 2 == 0 else \"blue\"\n",
    "            ax.axvspan(\n",
    "                xmin=gap[0],\n",
    "                xmax=gap[1],\n",
    "                color=color,\n",
    "                alpha=0.1,\n",
    "                label=\"Large gap in observations\" if i == 0 else None,\n",
    "            )\n",
    "        ax.grid()\n",
    "\n",
    "    axs[0].legend(ncol=5)\n",
    "    axs[0].set_ylabel(f\"{frame_name} Error [km]\")\n",
    "    axs[1].set_ylabel(f\"{frame_name} Error [km]\")\n",
    "    axs[0].set_xlabel(\"Year\")\n",
    "\n",
    "    axs[0].set_title(f\"Error vs SPICE over time for {target_name}\")\n",
    "    axs[1].set_title(f\"Error vs JPL HORIZONS over time for {target_name}\")\n",
    "    fig.suptitle(f\"Setup: {setup_name}\")\n",
    "    fig.set_tight_layout(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Round 1: Acceleration models\n",
    "With our core estimation and plotting functions ready, we can now perform a comparison of the three acceleration models. For this first comparison, we turn the remaining options: including satelite data, star catalog corrections and observations weights off. The setups can be described as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_names = [\"LVL1 Accelerations\", \"LVL2 Accelerations\", \"LVL3 Accelerations\"]\n",
    "\n",
    "accel_levels = [1, 2, 3]\n",
    "use_sat_data = [False, False, False]\n",
    "use_catalog_cor = [False, False, False]\n",
    "use_weighting = [False, False, False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the estimation\n",
    "We can then run the setups using `perform_estimation` to retrieve our pod_outputs, observation collections, estimator objects and also retrieve the state at a set of times for later comparison with SPICE and horizons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_output_set = []\n",
    "batch_set = []\n",
    "observation_collection_set = []\n",
    "estimator_set = []\n",
    "state_estimates_set = []\n",
    "\n",
    "for idx, setup_name in enumerate(setup_names):\n",
    "    print(f\"\\n### Running setup #{idx+1} | {setup_name} ###\")\n",
    "\n",
    "    pod_output, batch, observation_collection, estimator = (\n",
    "        perform_estimation(\n",
    "            bodies,\n",
    "            acceleration_level=accel_levels[idx],\n",
    "            use_satellite_data=use_sat_data[idx],\n",
    "            apply_star_catalog_debias=use_catalog_cor[idx],\n",
    "            apply_weighting_scheme=use_weighting[idx],\n",
    "        )\n",
    "    )\n",
    "    state_estimates = []\n",
    "    for timee in times_get_eph:\n",
    "        state_est = bodies.get(str(target_mpc_code)).ephemeris.cartesian_state(timee)\n",
    "        state_estimates.append(state_est)\n",
    "\n",
    "    pod_output_set.append(pod_output)\n",
    "    batch_set.append(batch)\n",
    "    observation_collection_set.append(observation_collection)\n",
    "    estimator_set.append(estimator)\n",
    "    state_estimates_set.append(state_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the results\n",
    "The result of the estimation is plotted below. The first plot shows similar residuals for all three setups, with all setups converging within 6 iteration. In terms of the cartesian errors, adding the additional moons in LVL2 greatly reduces the error, however additions added beyond that in LVL3 have almost no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(setup_names, pod_output_set, observation_collection_set)\n",
    "plot_cartesian(state_estimates_set, setup_names, observation_collection_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison Round 2: Weighting, Star Catalog Corrections and Satellite data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights and Star Catalog Biases\n",
    "Before running the next round, lets have a quick look at the star catalog corrections and observation weights which are based on the following literature:\n",
    "\n",
    "- \"Star catalog position and proper motion corrections in asteroid astrometry II: The Gaia era\" by Eggl et al.\n",
    "- \"Statistical analysis of astrometric errors for the most productive asteroid surveys\" by Veres et al.\n",
    "\n",
    "Star catalogs are large databases of distant celestial objects (mainly stars) featuring details about their position, motion and other properties. Catalogs are used as reference when making observations of objects such as asteroids. Many different catalogs exist each with slightly varying contents and accuracy. The Gaia space telescope, launched in 2013, was designed specifically to measure celestial objects with unprecedented precision. The emergence of the resulting Gaia star catalogs (first appearing in 2016) has made all previous catalogs obsolete, however, observations made with older catalogs still contain their errors. These errors are corrected per observation by enabling the `apply_star_catalog_debias` option in `BatchMPC.to_tudat()`.\n",
    "\n",
    "Additionally, not all observations have the same quality, to account for this we use weights to increase the effect of quality observations in our estimation. Specific observatories may have a higher accuracy, and individual observatories may improve their observation quality over time. Having too many observations by a single observatory in a short space of time may also introduce a heavy bias in the estimation. The work by Veres et al analyses the most prolific observatories to generate a weighting scheme which is enabled in Tudat using the `apply_star_catalog_debias` option in `BatchMPC.to_tudat()` and subsequently retrieving the weights in the pod_input using `pod_input.set_weights_from_observation_collection().\n",
    "`\n",
    "\n",
    "The plots below show star catalog corrections and observation weights for the observation period. Note in the star catalog correction graph how the number of corrections required (non-zero points) quickly reduces after 2016 once operators start implementing GAIA. Note also how a large clump of satellite observations in 2021 gets deweighted to prevent bias towards the satellite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = batch.copy()\n",
    "temp.to_tudat(bodies=bodies, included_satellites=None, apply_weights_VFCC17=True)\n",
    "# mark weights red if it is a satellite observation\n",
    "marker_color = [\"tab:red\" if x == \"S\" else \"tab:blue\" for x in temp.table.note2] \n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(9, 6), sharex=True)\n",
    "ax1.scatter(temp.table.epochUTC, temp.table.corr_RA_EFCC18, color=\"tab:blue\", marker=\"+\")\n",
    "ax2.scatter(temp.table.epochUTC, temp.table.corr_DEC_EFCC18, color=\"tab:orange\", marker=\"+\")\n",
    "\n",
    "ax1.set_ylabel(r\"Right Ascension $[rad]$\")\n",
    "ax2.set_ylabel(r\"Declination $[rad]$\")\n",
    "ax1.grid()\n",
    "ax2.grid()\n",
    "fig.suptitle(\"Star Catalog Corrections (per observation)\")\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9,4))\n",
    "ax.scatter(temp.table.epochUTC, temp.table.weight, marker=\"+\", c=marker_color)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_ylabel(r\"Weight $[rad^{-1}]$\")\n",
    "ax.grid()\n",
    "fig.suptitle(\"Observation Weights (per RA/DEC pair) (bigger = more impact) [red=Satellite]\")\n",
    "fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the comparison\n",
    "Lets now run some new setups with those features. As the difference between LVL2 and 3 accelerations is small, we use LVL 2 for all the remaining setups to save on runtime. Using level 2 as a baseline, we then succesively add the star catalog correction, observation weights and finally the satellite observations. The new estimation setups are defined below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_names_2 = [\n",
    "    \"LVL2\",\n",
    "    \"LVL2 + star catalog\",\n",
    "    \"LVL2 + star catalog + weighting\",\n",
    "    \"LVL2 + star catalog + weighting + Sat Data\",\n",
    "]\n",
    "\n",
    "accel_levels_2 = [2, 2, 2, 2]\n",
    "use_catalog_cor_2 = [False, True, True, True]\n",
    "use_weighting_2 = [False, False, True, True]\n",
    "use_sat_data_2 = [False, False, False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pod_output_set_2 = []\n",
    "batch_set_2 = []\n",
    "observation_collection_set_2 = []\n",
    "estimator_set_2 = []\n",
    "state_estimates_set_2 = []\n",
    "\n",
    "# This samples the cartesian state at 500 points over the observation time:\n",
    "times_get_eph = np.linspace(epoch_start_nobuffer, epoch_end_nobuffer, 500)\n",
    "\n",
    "for idx, setup_name in enumerate(setup_names_2):\n",
    "    print(f\"\\n### Running setup #{idx+1} | {setup_name} ###\")\n",
    "\n",
    "    pod_output, batch, observation_collection, estimator = perform_estimation(\n",
    "        bodies,\n",
    "        acceleration_level=accel_levels_2[idx],\n",
    "        use_satellite_data=use_sat_data_2[idx],\n",
    "        apply_star_catalog_debias=use_catalog_cor_2[idx],\n",
    "        apply_weighting_scheme=use_weighting_2[idx],\n",
    "    )\n",
    "    state_estimates = []\n",
    "    for timee in times_get_eph:\n",
    "        state_est = bodies.get(str(target_mpc_code)).ephemeris.cartesian_state(timee)\n",
    "        state_estimates.append(state_est)\n",
    "\n",
    "    pod_output_set_2.append(pod_output)\n",
    "    batch_set_2.append(batch)\n",
    "    observation_collection_set_2.append(observation_collection)\n",
    "    estimator_set_2.append(estimator)\n",
    "    state_estimates_set_2.append(state_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The results\n",
    "Before looking at the plots, lets look at the formal errors. We can see that the formal errors are reduced when the weights are applied, indicating that it is working.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, p_out in zip(setup_names_2, pod_output_set_2):\n",
    "    print(name, \" | \", list(p_out.formal_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the residual plot, the first three plots again appear indiscernable. The introduction of satellite data in the fourth image however clearly increases the magnitude of the residuals. This is also reflected in the Cartesian plot, indicating that the addition of satellite data has an adverse affect on the estimation in this scenario. Note that Tudat currently does not feature a outlier removal system, which would reduce the effect of singular outliers which are not captured by the weighting scheme. Additionally, the introduction of satellite data in a different scenario may have beneficial effects.\n",
    "\n",
    "Since the remaining setups do not show a strong difference, let take a closer look at the setup `LVL2 + star catalog + weighting` for the remainder of the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(setup_names_2, pod_output_set_2, observation_collection_set_2)\n",
    "plot_cartesian(state_estimates_set_2, setup_names_2, observation_collection_set_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Final setup\n",
    "Below we plot a more detailed version of the setup `LVL2 + star catalog + weighting` in both Cartesian and RSW frames and compared to both JPL Horizons and SPICE. From the first plot we can already clearly see that there is a strong difference between the error when compared to SPICE and Horizons. Eventhough we consider both to be \"ground-truth\" throughout the example, it is important to note that both systems are also estimations. Writers of the `CODES_300ast...` spice kernel, created in 2010, [recommend using Horizons](https://naif.jpl.nasa.gov/pub/naif/generic_kernels/spk/asteroids/AAREADME_Asteroids_SPKs.txt) for a more up to date ephemeris of asteroids.\n",
    "\n",
    "The accuracy of Horizons' ephemeris of 433 eros is given in the [SBDB](https://ssd.jpl.nasa.gov/tools/sbdb_lookup.html#/?sstr=433) gives a 1-sigma uncertainty of about 1.6e-10 AU = 24 meters for the semi major axis and about 1.2e-6 degrees in inclination (about 4.5 km at maximum). Looking at the error in the RSW frame shows that we are off in the order of 10s of kilometers. Indeed looking at the RSW error for Horizons we can clearly see a periodicity in the error. This indicates that there is potentially an acceleration that is modelled differently between our estimation and that of Horizons. We can also see hat the error is highest in the cross track direction. While the introduction of additional bodies in our acceleration models yielded no effects, it may be that Horizons models some of the mayor bodies (such as Jupiter) differently.\n",
    "\n",
    "Running the setup for a longer period of time and analysing the frequency domain may yield answers as to where the discrepancy lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_setup_index = 2\n",
    "# chosen_setup_index = 3 # consider trying index 3 to analyse at the satellite setup more closely.\n",
    "\n",
    "final_state_estimate = state_estimates_set_2[chosen_setup_index]\n",
    "final_setup_name = setup_names_2[chosen_setup_index]\n",
    "final_observation_collection = observation_collection_set_2[chosen_setup_index]\n",
    "final_estimator = estimator_set_2[chosen_setup_index]\n",
    "final_pod_output = pod_output_set_2[chosen_setup_index]\n",
    "\n",
    "print(f\"Final setup: {final_setup_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_cartesian_single(final_state_estimate, final_setup_name, final_observation_collection, in_RSW_frame=False)\n",
    "plot_cartesian_single(final_state_estimate, final_setup_name, final_observation_collection, in_RSW_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the main part of this example. Consider experimenting with the setup: using different space telescopes, trying out longer runtimes, different target bodies and different acceleration models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional plots\n",
    "Below are the same comparison plots used in the original example. Consider comparing the results to the previous example. Also consider running the changing the plots to the final setup with satellites to compare the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corellation can be retrieved using the CovarianceAnalysisInput class:\n",
    "covariance_input = estimation.CovarianceAnalysisInput(final_observation_collection)\n",
    "covariance_output = final_estimator.compute_covariance(covariance_input)\n",
    "\n",
    "correlations = covariance_output.correlations\n",
    "\n",
    "estimated_param_names = [\"x\", \"y\", \"z\", \"vx\", \"vy\", \"vz\"]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9, 7))\n",
    "\n",
    "im = ax.imshow(correlations, cmap=cm.RdYlBu_r, vmin=-1, vmax=1)\n",
    "\n",
    "ax.set_xticks(np.arange(len(estimated_param_names)), labels=estimated_param_names)\n",
    "ax.set_yticks(np.arange(len(estimated_param_names)), labels=estimated_param_names)\n",
    "\n",
    "# add numbers to each of the boxes\n",
    "for i in range(len(estimated_param_names)):\n",
    "    for j in range(len(estimated_param_names)):\n",
    "        text = ax.text(\n",
    "            j, i, round(correlations[i, j], 2), ha=\"center\", va=\"center\", color=\"w\"\n",
    "        )\n",
    "\n",
    "cb = plt.colorbar(im)\n",
    "\n",
    "ax.set_xlabel(\"Estimated Parameter\")\n",
    "ax.set_ylabel(\"Estimated Parameter\")\n",
    "\n",
    "fig.suptitle(f\"Correlations for estimated parameters for {target_name}\")\n",
    "\n",
    "fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final residuals highlighted per observatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_observatories = 10\n",
    "consider_satellites = use_sat_data_2[chosen_setup_index]\n",
    "\n",
    "residual_history = final_pod_output.residual_history\n",
    "residual_times = (\n",
    "    np.array(final_observation_collection.concatenated_times) / (86400 * 365.25) + 2000\n",
    ")\n",
    "finalresiduals = np.array(residual_history[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This piece of code collects the 10 largest observatories\n",
    "observatory_names = (\n",
    "    batch.observatories_table(exclude_space_telescopes=True)\n",
    "    .sort_values(\"count\", ascending=False)\n",
    "    .iloc[0:num_observatories]\n",
    "    .set_index(\"Code\")\n",
    ")\n",
    "top_observatories = observatory_names.index.tolist()\n",
    "# This piece of code creates a `concatenated_receiving_observatories` map\n",
    "# to identify the observatories by their MPC code instead of an internally used id\n",
    "residuals_observatories = final_observation_collection.concatenated_link_definition_ids\n",
    "unique_observatories = set(residuals_observatories)\n",
    "\n",
    "observatory_link_to_mpccode = {\n",
    "    idx: final_observation_collection.link_definition_ids[idx][\n",
    "        observation_collection.receiver\n",
    "    ].reference_point\n",
    "    for idx in unique_observatories\n",
    "}\n",
    "\n",
    "# the resulting map (MPC code for each item in the residuals_history):\n",
    "concatenated_receiving_observatories = np.array(\n",
    "    [observatory_link_to_mpccode[idx] for idx in residuals_observatories]\n",
    ")\n",
    "\n",
    "# mask for the observatories not in top 10:\n",
    "mask_not_top = [\n",
    "    (False if observatory in top_observatories else True)\n",
    "    for observatory in concatenated_receiving_observatories\n",
    "]\n",
    "\n",
    "# get the number of observations by the other observatories\n",
    "# (divide by two because the observations are concatenated RA,DEC in this list)\n",
    "n_obs_not_top = int(sum(mask_not_top) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(13, 9))\n",
    "\n",
    "# Plot remaining observatories first\n",
    "# RA\n",
    "axs[0].scatter(\n",
    "    residual_times[mask_not_top][::2],\n",
    "    finalresiduals[mask_not_top][::2],\n",
    "    marker=\".\",\n",
    "    s=30,\n",
    "    label=f\"{len(unique_observatories) - num_observatories} Other Observatories | {n_obs_not_top} obs\",\n",
    "    color=\"lightgrey\",\n",
    ")\n",
    "# DEC\n",
    "axs[1].scatter(\n",
    "    residual_times[mask_not_top][1::2],\n",
    "    finalresiduals[mask_not_top][1::2],\n",
    "    marker=\".\",\n",
    "    s=30,\n",
    "    label=f\"{len(unique_observatories) - num_observatories} Other Observatories | {n_obs_not_top} obs\",\n",
    "    color=\"lightgrey\",\n",
    ")\n",
    "\n",
    "# plots the highlighted top 10 observatories\n",
    "for observatory in top_observatories:\n",
    "    name = f\"{observatory} | {observatory_names.loc[observatory].Name} | {int(observatory_names.loc[observatory]['count'])} obs\"\n",
    "    axs[0].scatter(\n",
    "        residual_times[concatenated_receiving_observatories == observatory][::2],\n",
    "        finalresiduals[concatenated_receiving_observatories == observatory][::2],\n",
    "        marker=\".\",\n",
    "        s=30,\n",
    "        label=name,\n",
    "        zorder=100,\n",
    "    )\n",
    "    axs[1].scatter(\n",
    "        residual_times[concatenated_receiving_observatories == observatory][1::2],\n",
    "        finalresiduals[concatenated_receiving_observatories == observatory][1::2],\n",
    "        marker=\".\",\n",
    "        s=30,\n",
    "        label=name,\n",
    "        zorder=100,\n",
    "    )\n",
    "\n",
    "\n",
    "axs[1].legend(ncols=2, loc=\"upper center\", bbox_to_anchor=(0.47, -0.15))\n",
    "\n",
    "for ax in fig.get_axes():\n",
    "    ax.grid()\n",
    "    ax.set_ylabel(\"Observation Residual [rad]\")\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    # this step hides a few outliers (~3 observations)\n",
    "    ax.set_ylim(-1.5e-5, 1.5e-5)\n",
    "\n",
    "axs[0].set_title(\"Right Ascension\")\n",
    "axs[1].set_title(\"Declination\")\n",
    "\n",
    "fig.suptitle(f\"Final Iteration residuals for {target_name}\")\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms per observatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_observatories = 6\n",
    "nbins = 20\n",
    "number_of_columns = 2\n",
    "transparency = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows = (\n",
    "    int(num_observatories / number_of_columns)\n",
    "    if num_observatories % number_of_columns == 0\n",
    "    else int((num_observatories + 1) / number_of_columns)\n",
    ")\n",
    "\n",
    "# we retrieve the observatory names again\n",
    "observatory_names_hist = (\n",
    "    batch.observatories_table(exclude_space_telescopes=True)\n",
    "    .set_index(\"Code\")\n",
    "    .sort_values(\"count\", ascending=False)\n",
    "    .iloc[0:num_observatories]\n",
    ")\n",
    "\n",
    "top_observatories_hist = observatory_names_hist.index.tolist()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    number_of_rows,\n",
    "    number_of_columns,\n",
    "    figsize=(4.5 * number_of_columns, 3 * number_of_rows),\n",
    ")\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "for idx, observatory in enumerate(top_observatories_hist):\n",
    "    name = f\"{observatory} | {observatory_names_hist.loc[observatory].Name} | {int(observatory_names_hist.loc[observatory]['count'])} obs\"\n",
    "\n",
    "    axs[idx].hist(\n",
    "        finalresiduals[concatenated_receiving_observatories == observatory][0::2],\n",
    "        bins=nbins,\n",
    "        alpha=transparency + 0.05,\n",
    "        label=\"Right Ascension\",\n",
    "    )\n",
    "    axs[idx].hist(\n",
    "        finalresiduals[concatenated_receiving_observatories == observatory][1::2],\n",
    "        bins=nbins,\n",
    "        alpha=transparency,\n",
    "        label=\"Declination\",\n",
    "    )\n",
    "\n",
    "    axs[idx].grid()\n",
    "    axs[idx].set_title(name)\n",
    "    axs[idx].set_ylabel(\"Number of Observations\")\n",
    "    axs[idx].set_xlabel(\"Observation Residual [rad]\")\n",
    "\n",
    "axs[0].legend()\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"Final residual histograms of the {num_observatories} observatories with the most observations for {target_name}\"\n",
    ")\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
