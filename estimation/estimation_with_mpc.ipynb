{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial state estimation From Real MPC Observations\n",
    "\n",
    "## Objectives\n",
    "This example highlights a **simple orbit estimation routine** using **real angular observation data** from the  [Minor Planet Center](https://www.minorplanetcenter.net/) (MPC). \n",
    "\n",
    "In the [DELFI-C3 - Parameter Estimation Example](full_estimation_example.ipynb), we saw how to set up and run an **orbit estimation routine**, and we did so by using **simulated observational data**. While simulating observational data is certainly useful for a variety of purposes, in real life we want to estimate an orbit starting from **real data** coming from radio or optical observations.\n",
    "\n",
    "We will estimate the initial state of [Eros](https://en.wikipedia.org/wiki/433_Eros), a near-Earth asteroid also visited by the NEAR Shoemaker probe in 1998. We will use the `Tudat BatchMPC` interface to retrieve and process the data. For a more in depth explanation of this interface we recommend first checking out the [Retrieving observation data from the Minor Planet Centre](retrieving_mpc_observation_data.ipynb) example. We will also briefly use the SBDBquery class which interfaces JPL's [Small Body DataBase (SBDB)](https://ssd.jpl.nasa.gov/tools/sbdb_lookup.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements\n",
    "Let's start with importing the required modules. Most - if not all - of them (spice, numerical_simulation, environment, propagation) are used quite extensively in pretty much all tudatpy examples.They will soon become your friends (if they haven't already!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tudat imports for propagation and estimation\n",
    "from tudatpy.interface import spice\n",
    "from tudatpy import dynamics\n",
    "from tudatpy.dynamics import environment_setup\n",
    "from tudatpy.dynamics import propagation_setup, parameters_setup, simulator\n",
    "from tudatpy import estimation\n",
    "from tudatpy.estimation import observable_models_setup, observable_models, observations_setup, observations, estimation_analysis\n",
    "from tudatpy.astro.time_representation import DateTime\n",
    "\n",
    "# import MPC interface\n",
    "from tudatpy.data.mpc import BatchMPC\n",
    "\n",
    "# import SBDB interface\n",
    "from tudatpy.data.sbdb import SBDBquery\n",
    "\n",
    "# other useful modules\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the environment and observations\n",
    "\n",
    "### Loading Spice Kernels.\n",
    "We use SPICE kernels to retrieve the ephemerides the planets as well as to verify our results for Eros. The ephemerides for Eros and other asteroids are loaded in with the `codes_300ast_20100725.bsp` kernel included with Tudat's standard kernels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPICE KERNELS\n",
    "spice.load_standard_kernels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting some constants\n",
    "Let's setup some constants that are used throughout the tutorial. The **MPC code** for Eros is 433. We also set a start and end date for our observations, the number of iterations for our estimation, a timestep for our integrator and a 1 month buffer to avoid interpolation errors in our analysis.\n",
    "\n",
    "We use a spice kernel to get a guess for our initial state and to check our estimation afterwards. The default spice kernel `codes_300ast_20100725.bsp` contains many popular asteroids, however they are not all identified by name (433 Eros is `\"Eros\"` but 16 Psyche is `\"2000016\"` etc.). To ensure this example works dynamically, for any single MPC code as input we use the SDBD to retrieve the name and SPK-ID used for the spice kernel.\n",
    "\n",
    "For our frame origin we use the Solar System Barycenter. The data from MPC is presented in the J2000 reference frame, currently BatchMPC does not support conversion to other reference frames and as such we match it in our environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_mpc_code = 433\n",
    "\n",
    "observations_start = datetime.datetime(2018, 1, 1)\n",
    "observations_end = datetime.datetime(2023, 7, 1)\n",
    "\n",
    "# number of iterations for our estimation\n",
    "number_of_pod_iterations = 6\n",
    "\n",
    "# timestep of 20 hours for our estimation\n",
    "timestep_global = 20 * 3600\n",
    "\n",
    "# 1 month time buffer used to avoid interpolation errors:\n",
    "time_buffer = 1 * 31 * 86400\n",
    "\n",
    "# define the frame origin and orientation.\n",
    "global_frame_origin = \"SSB\"\n",
    "global_frame_orientation = \"J2000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derived inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPK ID for 433 Eros is: Eros\n"
     ]
    }
   ],
   "source": [
    "target_sbdb = SBDBquery(target_mpc_code)\n",
    "\n",
    "mpc_codes = [target_mpc_code]  # the BatchMPC interface requires a list.\n",
    "target_spkid = target_sbdb.codes_300_spkid  # the ID used by the\n",
    "target_name = target_sbdb.shortname  # the ID used by the\n",
    "\n",
    "print(f\"SPK ID for {target_name} is: {target_spkid}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the observations\n",
    "We retrieve the observation data using the `BatchMPC` interface. By default, all observation data is retrieved (even the first observations from Witt in 1898!). We filter to only include data between our start and end dates. The command `batch.summary()` gives us a nice summary of the observations we just retrieved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   Batch Summary:\n",
      "1. Batch includes 1 minor planets:\n",
      "   ['433']\n",
      "2. Batch includes 3111 observations, including 1735 observations from space telescopes\n",
      "3. The observations range from 2018-05-01 03:22:18.336012 to 2023-06-27 00:37:43.680017\n",
      "   In seconds TDB since J2000: 578417007.5214744 to 741098332.8642726\n",
      "   In Julian Days: 2458239.64049 to 2460122.5262\n",
      "4. The batch contains observations from 54 observatories, including 4 space telescopes\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/tudat-bundle/lib/python3.11/site-packages/tudatpy/data/mpc/mpc.py:863: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['433' '433' '433' ... '433' '433' '433']' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  obs.loc[:, \"number\"] = obs.number.astype(str)\n"
     ]
    }
   ],
   "source": [
    "batch = BatchMPC()\n",
    "batch.get_observations(mpc_codes)\n",
    "batch.filter(\n",
    "    epoch_start=observations_start,\n",
    "    epoch_end=observations_end,\n",
    ")\n",
    "\n",
    "batch.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than **Earth-based telescopes**, our batch also includes observations from **space telescopes**.\n",
    "Let's check that out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary of space telescopes in batch:\n",
      "     Code                                    Name   count\n",
      "274   275  Non-geocentric Occultation Observation     2.0\n",
      "1232  C51                                    WISE   111.0\n",
      "1238  C57                                    TESS  1620.0\n",
      "1240  C59                              Yangwang-1     2.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Summary of space telescopes in batch:\")\n",
    "print(batch.observatories_table(only_space_telescopes=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, observations by WISE, TESS and Yangwang, as well as some non-geocentric Occulation Observations are found. We can exemplary plot the initial and final observations of both TESS and WISE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial and Final Observations by WISE:\n",
      "      number                   epochUTC        RA       DEC\n",
      "10833    433 2018-07-25 00:09:44.063981  0.468337  0.388395\n",
      "13848    433 2023-05-15 12:31:37.344004  5.690128 -0.311711\n"
     ]
    }
   ],
   "source": [
    "obs_by_WISE = (\n",
    "    batch.table.query(\"observatory == 'C51'\")\n",
    "    .loc[:, [\"number\", \"epochUTC\", \"RA\", \"DEC\"]]\n",
    "    .iloc[[0, -1]]\n",
    ")\n",
    "\n",
    "print(\"\\nInitial and Final Observations by WISE:\")\n",
    "print(obs_by_WISE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the observations from space telescopes appear to be useful, including them requires setting up the dynamics for the spacecraft, which is too advanced for this tutorial. Space-based observations will therefore be excluded later on in this example. \n",
    "\n",
    "Also note that if, for any reason, you would like to filter out some other observations, you can do so by excluding the observatories with the `.filter()` method, specifying their codes (for instance, use `.filter('C59')` will filter out observations from Yangwang-1). Note that all observations give Right Ascension (RA) and Declination (DEC) in **radians**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the environment\n",
    "We now set up the environment, including the bodies to use, the reference frame and frame origin. The ephemerides for all major planets as well as the Earth's Moon are retrieved using spice. \n",
    "\n",
    "BatchMPC will automatically generate the body object for Eros, but we still need to specify the bodies to propagate and their central bodies. We can retrieve the list from the BatchMPC object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the bodies for our environment\n",
    "bodies_to_create = [\n",
    "    \"Sun\",\n",
    "    \"Mercury\",\n",
    "    \"Venus\",\n",
    "    \"Earth\",\n",
    "    \"Moon\",\n",
    "    \"Mars\",\n",
    "    \"Jupiter\",\n",
    "    \"Saturn\",\n",
    "    \"Uranus\",\n",
    "    \"Neptune\",\n",
    "]\n",
    "\n",
    "# Create system of bodies\n",
    "body_settings = environment_setup.get_default_body_settings(\n",
    "    bodies_to_create, global_frame_origin, global_frame_orientation\n",
    ")\n",
    "\n",
    "bodies = environment_setup.create_system_of_bodies(body_settings)\n",
    "\n",
    "# Retrieve Eros' body name from BatchMPC and set its centre to enable its propagation\n",
    "bodies_to_propagate = batch.MPC_objects\n",
    "central_bodies = [global_frame_origin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the observations to Tudat\n",
    "Now that our system of bodies is ready, we can retrieve the observation collection from the observations batch using the `to_tudat()` method. By setting the `included_satellites` to `None`, we filter out all space-based observations. From the **observation collection** we can also retrieve **observation links**. As you already know from [Covariance estimation example](covariance_estimated_parameters.ipynb), we use the links to define our **observations settings**. This is also where you would add the **bias settings**. For the purpose of this example, we will use the plain **angular position observation settings**, which can process observations with Right Ascension and Declination. We can also retrieve the times for the first and final observations from the batch object in seconds since J2000 TDB, which is what tudat uses internally. We here add our buffer, set previously, to avoid interpolation errors down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__(): incompatible constructor arguments. The following argument types are supported:\n    1. tudatpy.kernel.astro.time_representation.DateTime(year: typing.SupportsInt, month: typing.SupportsInt, day: typing.SupportsInt, hour: typing.SupportsInt = 12, minute: typing.SupportsInt = 0, seconds: typing.SupportsFloat = 0.0)\n\nInvoked with: 578417007.5214744",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     15\u001b[39m     observation_settings_list.append(\n\u001b[32m     16\u001b[39m         observable_models_setup.model_settings.angular_position(link, bias_settings=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     17\u001b[39m     )\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Retrieve the first and final observation epochs and add the buffer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m epoch_start_nobuffer = \u001b[43mDateTime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mepoch_start\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m epoch_end_nobuffer = batch.epoch_end\n\u001b[32m     22\u001b[39m epoch_start_buffer = epoch_start_nobuffer - time_buffer\n",
      "\u001b[31mTypeError\u001b[39m: __init__(): incompatible constructor arguments. The following argument types are supported:\n    1. tudatpy.kernel.astro.time_representation.DateTime(year: typing.SupportsInt, month: typing.SupportsInt, day: typing.SupportsInt, hour: typing.SupportsInt = 12, minute: typing.SupportsInt = 0, seconds: typing.SupportsFloat = 0.0)\n\nInvoked with: 578417007.5214744"
     ]
    }
   ],
   "source": [
    "# Transform the MPC observations into a tudat compatible format.\n",
    "# note that we explicitly exclude all satellite observations in this step by setting included satellites to None.\n",
    "observation_collection = batch.to_tudat(bodies=bodies, included_satellites=None)\n",
    "\n",
    "# set create angular_position settings for each link in the list.\n",
    "observation_settings_list = list()\n",
    "link_list = list(\n",
    "    observation_collection.get_link_definitions_for_observables(\n",
    "        observable_type=observable_models_setup.model_settings.angular_position_type\n",
    "    )\n",
    ")\n",
    "\n",
    "for link in link_list:\n",
    "    # add optional bias settings here\n",
    "    observation_settings_list.append(\n",
    "        observable_models_setup.model_settings.angular_position(link, bias_settings=None)\n",
    "    )\n",
    "# Retrieve the first and final observation epochs and add the buffer\n",
    "epoch_start_nobuffer = batch.epoch_start\n",
    "epoch_end_nobuffer = batch.epoch_end\n",
    "\n",
    "epoch_start_buffer = epoch_start_nobuffer - time_buffer\n",
    "epoch_end_buffer = epoch_end_nobuffer + time_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the acceleration settings\n",
    "In order to estimate the orbit of Eros, we need to **propagate its initial state**. The propagation can only be performed upon definition of a **dynamical model**. Thus, we need to define the settings of the forces acting on Eros, which will determine its trajectory. We will include:\n",
    "\n",
    "* **point mass gravity accelerations** for each of the bodies defined before,\n",
    "* Schwarzschild **relativistic corrections** for the Sun.\n",
    "\n",
    "With these accelerations we can generate our **acceleration model for the propagation**. A more realistic acceleration model will yield better results but this is outside the scope of this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define accelerations\n",
    "accelerations = {\n",
    "    \"Sun\": [\n",
    "        propagation_setup.acceleration.point_mass_gravity(),\n",
    "        propagation_setup.acceleration.relativistic_correction(use_schwarzschild=True),\n",
    "    ],\n",
    "    \"Mercury\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Venus\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Earth\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Moon\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Mars\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Jupiter\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Saturn\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Uranus\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "    \"Neptune\": [propagation_setup.acceleration.point_mass_gravity()],\n",
    "}\n",
    "\n",
    "# Set up the accelerations settings for each body, in this case only Eros\n",
    "acceleration_settings = {}\n",
    "for body in batch.MPC_objects:\n",
    "    acceleration_settings[str(body)] = accelerations\n",
    "\n",
    "# create the acceleration models.\n",
    "acceleration_models = propagation_setup.create_acceleration_models(\n",
    "    bodies, acceleration_settings, bodies_to_propagate, central_bodies\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving an initial guess for Eros' position\n",
    "As we mentioned above, we need to propagate an initial state. We use the `SPICE` ephemeris to retrieve a 'benchmark' initial state for Eros at the epoch at the propagation start epoch. We can also use this initial state to set our **initial guess for the estimation**. To define the initial guess, we add a **random uniform offset** of +/- 1 million kilometers for the position and 100 m/s for the velocity. Adding this random offset to the `SPICE` initial state should not have a strong influence on the final results, and it is added in order to keep the tutorial representative (in real-world cases we might not have such a good initial guess!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmark state for later comparison retrieved from SPICE\n",
    "initial_states = spice.get_body_cartesian_state_at_epoch(\n",
    "    target_spkid,\n",
    "    global_frame_origin,\n",
    "    global_frame_orientation,\n",
    "    \"NONE\",\n",
    "    epoch_start_buffer,\n",
    ")\n",
    "\n",
    "# Add random offset for initial guess\n",
    "rng = np.random.default_rng(seed=1)\n",
    "\n",
    "initial_position_offset = 1e6 * 1000\n",
    "initial_velocity_offset = 100\n",
    "\n",
    "initial_guess = initial_states.copy()\n",
    "initial_guess[0:3] += (2 * rng.random(3) - 1) * initial_position_offset\n",
    "initial_guess[3:6] += (2 * rng.random(3) - 1) * initial_velocity_offset\n",
    "\n",
    "print(\"Error between the real initial state and our initial guess:\")\n",
    "print(initial_guess - initial_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalising the propagation setup\n",
    "For the integrator we use the fixed timestep RKF-7(8) setting our initial time to the time of the batch's final observation - buffer. We then set the termination to stop at the time of the batch's oldest observation plus buffer. These two settings are then the final pieces to create our propagation settings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numerical integrator settings\n",
    "integrator_settings = propagation_setup.integrator.runge_kutta_variable_step_size(\n",
    "    epoch_start_buffer,\n",
    "    timestep_global,\n",
    "    propagation_setup.integrator.CoefficientSets.rkf_78,\n",
    "    timestep_global,\n",
    "    timestep_global,\n",
    "    1.0,\n",
    "    1.0,\n",
    ")\n",
    "\n",
    "# Terminate at the time of oldest observation\n",
    "termination_condition = propagation_setup.propagator.time_termination(epoch_end_buffer)\n",
    "\n",
    "\n",
    "# Create propagation settings\n",
    "propagator_settings = propagation_setup.propagator.translational(\n",
    "    central_bodies=central_bodies,\n",
    "    acceleration_models=acceleration_models,\n",
    "    bodies_to_integrate=bodies_to_propagate,\n",
    "    initial_states=initial_guess,\n",
    "    initial_time=epoch_start_buffer,\n",
    "    integrator_settings=integrator_settings,\n",
    "    termination_settings=termination_condition,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the estimation\n",
    "With the observation collection, the environment and propagations settings ready we can now begin setting up our estimation. \n",
    "In this example **we will simply estimate the position of Eros** and as such only include an **initial states parameter**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup parameters settings to propagate the state transition matrix\n",
    "parameter_settings = parameters_setup.parameter.initial_states(\n",
    "    propagator_settings, bodies\n",
    ")\n",
    "\n",
    "# Create the parameters that will be estimated\n",
    "parameters_to_estimate = parameters_setup.create_parameter_set(\n",
    "    parameter_settings, bodies, propagator_settings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Estimator` object collects the environment, observation settings and propagation settings. We also create an `EstimationInput` object and provide it our observation collection retrieved from `.to_tudat()`. Our maximum iterations steps was set to 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the estimator\n",
    "estimator = estimation_analysis.Estimator(\n",
    "    bodies=bodies,\n",
    "    estimated_parameters=parameters_to_estimate,\n",
    "    observation_settings=observation_settings_list,\n",
    "    propagator_settings=propagator_settings,\n",
    "    integrate_on_creation=True,\n",
    ")\n",
    "\n",
    "# provide the observation collection as input, and limit number of iterations for estimation.\n",
    "pod_input = estimation_analysis.EstimationInput(\n",
    "    observations_and_times=observation_collection,\n",
    "    convergence_checker=estimation.estimation_convergence_checker(\n",
    "        maximum_iterations=number_of_pod_iterations,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Set methodological options\n",
    "pod_input.define_estimation_settings(reintegrate_variational_equations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing the estimation\n",
    "\n",
    "With everything set up, we can now perform the estimation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the estimation\n",
    "pod_output = estimator.perform_estimation(pod_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the **residual values** after each iteration, the estimator appears to converge within ~4 steps. Lets check how close our **initial state guess** and the **final estimate for the initial state** are, compared to the benchmark initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the estimated initial state.\n",
    "results_final = pod_output.parameter_history[:, -1]\n",
    "\n",
    "vector_error_initial = (np.array(initial_guess) - initial_states)[0:3]\n",
    "error_magnitude_initial = np.sqrt(np.square(vector_error_initial).sum()) / 1000\n",
    "\n",
    "vector_error_final = (np.array(results_final) - initial_states)[0:3]\n",
    "error_magnitude_final = np.sqrt(np.square(vector_error_final).sum()) / 1000\n",
    "\n",
    "print(\n",
    "    f\"{target_name} initial guess radial error to spice: {round(error_magnitude_initial, 2)} km\"\n",
    ")\n",
    "print(\n",
    "    f\"{target_name} final radial error to spice: {round(error_magnitude_final, 2)} km\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising the results\n",
    "\n",
    "### Change in residuals per iteration\n",
    "We want to visualise the residuals, splitting them between Right Ascension and Declination. Internally, `concatenated_observations` orders the observations alternating RA, DEC, RA, DEC,... This allows us to map the colors accordingly by taking every other item in the `residual_history`/`concatenated_observations`, i.e. by slicing [::2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_history = pod_output.residual_history\n",
    "\n",
    "# Number of columns and rows for our plot\n",
    "number_of_columns = 2\n",
    "\n",
    "number_of_rows = (\n",
    "    int(number_of_pod_iterations / number_of_columns)\n",
    "    if number_of_pod_iterations % number_of_columns == 0\n",
    "    else int((number_of_pod_iterations + 1) / number_of_columns)\n",
    ")\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    number_of_rows,\n",
    "    number_of_columns,\n",
    "    figsize=(9, 3.5 * number_of_rows),\n",
    "    sharex=True,\n",
    "    sharey=False,\n",
    ")\n",
    "\n",
    "# We cheat a little to get an approximate year out of our times (which are in seconds since J2000)\n",
    "residual_times = (\n",
    "    np.array(observation_collection.concatenated_times) / (86400 * 365.25) + 2000\n",
    ")\n",
    "\n",
    "\n",
    "# plot the residuals, split between RA and DEC types\n",
    "for idx, ax in enumerate(fig.get_axes()):\n",
    "    ax.grid()\n",
    "    # we take every second\n",
    "    ax.scatter(\n",
    "        residual_times[::2],\n",
    "        residual_history[\n",
    "            ::2,\n",
    "            idx,\n",
    "        ],\n",
    "        marker=\"+\",\n",
    "        s=60,\n",
    "        label=\"Right Ascension\",\n",
    "    )\n",
    "    ax.scatter(\n",
    "        residual_times[1::2],\n",
    "        residual_history[\n",
    "            1::2,\n",
    "            idx,\n",
    "        ],\n",
    "        marker=\"+\",\n",
    "        s=60,\n",
    "        label=\"Declination\",\n",
    "    )\n",
    "    ax.set_ylabel(\"Observation Residual [rad]\")\n",
    "    ax.set_title(\"Iteration \" + str(idx + 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# add the year label for the x-axis\n",
    "for col in range(number_of_columns):\n",
    "    axs[int(number_of_rows - 1), col].set_xlabel(\"Year\")\n",
    "\n",
    "axs[0, 0].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals Correlations Matrix\n",
    "Lets check out the correlation of the estimated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation can be retrieved using the CovarianceAnalysisInput class:\n",
    "covariance_input = estimation_analysis.CovarianceAnalysisInput(observation_collection)\n",
    "covariance_output = estimator.compute_covariance(covariance_input)\n",
    "\n",
    "correlations = covariance_output.correlations\n",
    "estimated_param_names = [\"x\", \"y\", \"z\", \"vx\", \"vy\", \"vz\"]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9, 7))\n",
    "\n",
    "im = ax.imshow(correlations, cmap=cm.RdYlBu_r, vmin=-1, vmax=1)\n",
    "\n",
    "ax.set_xticks(np.arange(len(estimated_param_names)), labels=estimated_param_names)\n",
    "ax.set_yticks(np.arange(len(estimated_param_names)), labels=estimated_param_names)\n",
    "\n",
    "# add numbers to each of the boxes\n",
    "for i in range(len(estimated_param_names)):\n",
    "    for j in range(len(estimated_param_names)):\n",
    "        text = ax.text(\n",
    "            j, i, round(correlations[i, j], 2), ha=\"center\", va=\"center\", color=\"w\"\n",
    "        )\n",
    "\n",
    "cb = plt.colorbar(im)\n",
    "\n",
    "ax.set_xlabel(\"Estimated Parameter\")\n",
    "ax.set_ylabel(\"Estimated Parameter\")\n",
    "\n",
    "fig.suptitle(f\"Correlations for estimated parameters for {target_name}\")\n",
    "\n",
    "fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orbit error vs spice over time\n",
    "Next, lets take a look at the error of the orbit over time, using spice as a reference.\n",
    "\n",
    "We saw in the residuals graph that there are two large gaps in observations, for 2022 and around Jan 2020. Lets collect those gaps and overlay them on to our error plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets get ranges for all gaps larger than 6 months:\n",
    "gap_in_months = 6\n",
    "\n",
    "gaps = np.abs(np.diff(sorted(residual_times)))\n",
    "num_gaps = (\n",
    "    gaps > (gap_in_months / 12)\n",
    ").sum()  # counts the number of gaps larger than 0.5 years\n",
    "indices_of_largest_gaps = np.argsort(gaps)[-num_gaps:]\n",
    "\n",
    "# (start, end) for each of the gaps\n",
    "gap_ranges = [\n",
    "    (sorted(residual_times)[idx - 1], sorted(residual_times)[idx + 1])\n",
    "    for idx in indices_of_largest_gaps\n",
    "]\n",
    "\n",
    "print(f\"Largest gap = {round(max(gaps), 3)} years\")\n",
    "print(gap_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets plot the orbit error\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9, 5))\n",
    "\n",
    "# show areas where there are no observations:\n",
    "for i, gap in enumerate(gap_ranges):\n",
    "    ax.axvspan(\n",
    "        xmin=gap[0],\n",
    "        xmax=gap[1],\n",
    "        color=\"red\",\n",
    "        alpha=0.1,\n",
    "        label=\"Large gap in observations\" if i == 0 else None,\n",
    "    )\n",
    "\n",
    "spice_states = []\n",
    "estimation_states = []\n",
    "\n",
    "# retrieve the states for a list of times.\n",
    "times = np.linspace(epoch_start_nobuffer, epoch_end_nobuffer, 1000)\n",
    "times_plot = times / (86400 * 365.25) + 2000  # approximate\n",
    "for time in times:\n",
    "    # from spice\n",
    "    state_spice = spice.get_body_cartesian_state_at_epoch(\n",
    "        target_spkid, central_bodies[0], global_frame_orientation, \"NONE\", time\n",
    "    )\n",
    "    spice_states.append(state_spice)\n",
    "\n",
    "    # from estimation\n",
    "    state_est = bodies.get(str(target_mpc_code)).ephemeris.cartesian_state(time)\n",
    "    estimation_states.append(state_est)\n",
    "\n",
    "# Error in kilometers\n",
    "error = (np.array(spice_states) - np.array(estimation_states)) / 1000\n",
    "\n",
    "# plot\n",
    "ax.plot(times_plot, error[:, 0], label=\"x\")\n",
    "ax.plot(times_plot, error[:, 1], label=\"y\")\n",
    "ax.plot(times_plot, error[:, 2], label=\"z\")\n",
    "\n",
    "ax.grid()\n",
    "ax.legend(ncol=1)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "ax.set_ylabel(\"Cartesian Error [km]\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "\n",
    "fig.suptitle(f\"Error vs SPICE over time for {target_name}\")\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that a lack of observations in an area of time does not necessarily result in a bad fit in that area. Lets look at the observatories next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final residuals highlighted per observatory\n",
    "This plot shows the final iteration of the residuals, highlighting the 10 observatories with the most observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 observatories with most observations\n",
    "num_observatories = 10\n",
    "\n",
    "final_residuals = np.array(residual_history[:, -1])\n",
    "# if you would like to check the iteration 1 residuals, use:\n",
    "# final_residuals = np.array(residual_history[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This piece of code collects the 10 largest observatories\n",
    "observatory_names = (\n",
    "    batch.observatories_table(exclude_space_telescopes=True)\n",
    "    .sort_values(\"count\", ascending=False)\n",
    "    .iloc[0:num_observatories]\n",
    "    .set_index(\"Code\")\n",
    ")\n",
    "top_observatories = observatory_names.index.tolist()\n",
    "\n",
    "# This piece of code creates a `concatenated_receiving_observatories` map\n",
    "# to identify the observatories by their MPC code instead of an internally used id\n",
    "residuals_observatories = observation_collection.concatenated_link_definition_ids\n",
    "unique_observatories = set(residuals_observatories)\n",
    "\n",
    "observatory_link_to_mpccode = {\n",
    "    idx: observation_collection.link_definition_ids[idx][\n",
    "        observable_models_setup.links.receiver\n",
    "    ].reference_point\n",
    "    for idx in unique_observatories\n",
    "}\n",
    "\n",
    "# the resulting map (MPC code for each item in the residuals_history):\n",
    "concatenated_receiving_observatories = np.array(\n",
    "    [observatory_link_to_mpccode[idx] for idx in residuals_observatories]\n",
    ")\n",
    "\n",
    "# mask for the observatories not in top 10:\n",
    "mask_not_top = [\n",
    "    (False if observatory in top_observatories else True)\n",
    "    for observatory in concatenated_receiving_observatories\n",
    "]\n",
    "\n",
    "# get the number of observations by the other observatories\n",
    "# (divide by two because the observations are concatenated RA,DEC in this list)\n",
    "n_obs_not_top = int(sum(mask_not_top) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(13, 9))\n",
    "\n",
    "# Plot remaining observatories first\n",
    "# RA\n",
    "axs[0].scatter(\n",
    "    residual_times[mask_not_top][::2],\n",
    "    final_residuals[mask_not_top][::2],\n",
    "    marker=\".\",\n",
    "    s=30,\n",
    "    label=f\"{len(unique_observatories) - num_observatories} Other Observatories | {n_obs_not_top} obs\",\n",
    "    color=\"lightgrey\",\n",
    ")\n",
    "# DEC\n",
    "axs[1].scatter(\n",
    "    residual_times[mask_not_top][1::2],\n",
    "    final_residuals[mask_not_top][1::2],\n",
    "    marker=\".\",\n",
    "    s=30,\n",
    "    label=f\"{len(unique_observatories) - num_observatories} Other Observatories | {n_obs_not_top} obs\",\n",
    "    color=\"lightgrey\",\n",
    ")\n",
    "\n",
    "# plots the highlighted top 10 observatories\n",
    "for observatory in top_observatories:\n",
    "    name = f\"{observatory} | {observatory_names.loc[observatory].Name} | {int(observatory_names.loc[observatory]['count'])} obs\"\n",
    "    axs[0].scatter(\n",
    "        residual_times[concatenated_receiving_observatories == observatory][::2],\n",
    "        final_residuals[concatenated_receiving_observatories == observatory][::2],\n",
    "        marker=\".\",\n",
    "        s=30,\n",
    "        label=name,\n",
    "        zorder=100,\n",
    "    )\n",
    "    axs[1].scatter(\n",
    "        residual_times[concatenated_receiving_observatories == observatory][1::2],\n",
    "        final_residuals[concatenated_receiving_observatories == observatory][1::2],\n",
    "        marker=\".\",\n",
    "        s=30,\n",
    "        label=name,\n",
    "        zorder=100,\n",
    "    )\n",
    "\n",
    "\n",
    "axs[1].legend(ncols=3, loc=\"upper center\", bbox_to_anchor=(0.47, -0.15))\n",
    "\n",
    "for ax in fig.get_axes():\n",
    "    ax.grid()\n",
    "    ax.set_ylabel(\"Observation Residual [rad]\")\n",
    "    ax.set_xlabel(\"Year\")\n",
    "    # this step hides a few outliers (~3 observations)\n",
    "    ax.set_ylim(-1.5e-5, 1.5e-5)\n",
    "\n",
    "axs[0].set_title(\"Right Ascension\")\n",
    "axs[1].set_title(\"Declination\")\n",
    "\n",
    "fig.suptitle(f\"Final Iteration residuals for {target_name}\")\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Boxplots per observatory\n",
    "Let's visualise these residuals as boxplots as well, again splitting for right ascension and declination. Note that some low level Matplotlib is used for this plot. Consider using the simplified [seaborn boxplot](https://seaborn.pydata.org/generated/seaborn.boxplot.html) implementation if this format is relevant to your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_observatories = 6\n",
    "\n",
    "data_per_observatory_list_RA = []\n",
    "data_per_observatory_list_DEC = []\n",
    "names = []\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 1.2 * num_observatories))\n",
    "\n",
    "# we retrieve the observatory names again\n",
    "observatory_names_box = (\n",
    "    batch.observatories_table(exclude_space_telescopes=True)\n",
    "    .set_index(\"Code\")\n",
    "    .sort_values(\"count\", ascending=False)\n",
    "    .iloc[0:num_observatories]\n",
    ")\n",
    "\n",
    "top_observatories_box = observatory_names_box.index.tolist()\n",
    "\n",
    "# retrieve the data for RA and DEC separately\n",
    "for observatory in top_observatories_box[::-1]:\n",
    "    name = f\"{observatory} | {observatory_names_box.loc[observatory].Name} | {int(observatory_names_box.loc[observatory]['count'])} obs\"\n",
    "    names.append(name)\n",
    "    data_per_observatory_list_RA.append(\n",
    "        final_residuals[concatenated_receiving_observatories == observatory][::2]\n",
    "    )\n",
    "\n",
    "    data_per_observatory_list_DEC.append(\n",
    "        final_residuals[concatenated_receiving_observatories == observatory][1::2]\n",
    "    )\n",
    "\n",
    "# positioning the boxes\n",
    "pos = (np.arange(0, len(top_observatories_box)) + 1) * 10\n",
    "widths = 2.8\n",
    "offset = 1.6\n",
    "\n",
    "# box colors\n",
    "RA_color = \"tab:blue\"\n",
    "DEC_color = \"tab:orange\"\n",
    "\n",
    "# boxes for RA and DEC\n",
    "RAplots = ax.boxplot(\n",
    "    x=data_per_observatory_list_RA,\n",
    "    vert=False,\n",
    "    positions=pos + offset,\n",
    "    widths=widths,\n",
    "    patch_artist=False,\n",
    "    capprops=dict(color=RA_color),\n",
    "    whiskerprops=dict(color=RA_color),\n",
    "    flierprops=dict(color=RA_color, markeredgecolor=RA_color),\n",
    "    medianprops=dict(color=RA_color),\n",
    ")\n",
    "DECplots = ax.boxplot(\n",
    "    x=data_per_observatory_list_DEC,\n",
    "    vert=False,\n",
    "    positions=pos - offset,\n",
    "    widths=widths,\n",
    "    patch_artist=False,\n",
    "    capprops=dict(color=DEC_color),\n",
    "    whiskerprops=dict(color=DEC_color),\n",
    "    flierprops=dict(color=DEC_color, markeredgecolor=DEC_color),\n",
    "    medianprops=dict(color=DEC_color),\n",
    ")\n",
    "# custom ticks\n",
    "ax.set_yticks(ticks=pos, labels=names)\n",
    "\n",
    "# custom legend\n",
    "place_holder_lines = [\n",
    "    Line2D([0], [0], color=RA_color, lw=4),\n",
    "    Line2D([0], [0], color=DEC_color, lw=4),\n",
    "]\n",
    "ax.legend(place_holder_lines, [\"Right Ascension\", \"Declination\"])\n",
    "\n",
    "ax.grid()\n",
    "ax.set_xlabel(\"Observation Residual [rad]\")\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"Residual boxplots of the {num_observatories} observatories with the most observations for {target_name}\"\n",
    ")\n",
    "\n",
    "# reducing whitespace\n",
    "ax.set_ylim(10 - 4, int(len(top_observatories_box) * 10) + 4)\n",
    "\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms per observatory\n",
    "Finally, lets get the residual histogram for the top 6 observatories, splitting again for right ascension and declination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_observatories = 6\n",
    "nbins = 20\n",
    "number_of_columns = 2\n",
    "transparency = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_rows = (\n",
    "    int(num_observatories / number_of_columns)\n",
    "    if num_observatories % number_of_columns == 0\n",
    "    else int((num_observatories + 1) / number_of_columns)\n",
    ")\n",
    "\n",
    "# we retrieve the observatory names again\n",
    "observatory_names_hist = (\n",
    "    batch.observatories_table(exclude_space_telescopes=True)\n",
    "    .set_index(\"Code\")\n",
    "    .sort_values(\"count\", ascending=False)\n",
    "    .iloc[0:num_observatories]\n",
    ")\n",
    "\n",
    "top_observatories_hist = observatory_names_hist.index.tolist()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    number_of_rows,\n",
    "    number_of_columns,\n",
    "    figsize=(4.5 * number_of_columns, 3 * number_of_rows),\n",
    ")\n",
    "\n",
    "axs = axs.flatten()\n",
    "\n",
    "for idx, observatory in enumerate(top_observatories_hist):\n",
    "    name = f\"{observatory} | {observatory_names_hist.loc[observatory].Name} | {int(observatory_names_hist.loc[observatory]['count'])} obs\"\n",
    "\n",
    "    axs[idx].hist(\n",
    "        final_residuals[concatenated_receiving_observatories == observatory][0::2],\n",
    "        bins=nbins,\n",
    "        alpha=transparency + 0.05,\n",
    "        label=\"Right Ascension\",\n",
    "    )\n",
    "    axs[idx].hist(\n",
    "        final_residuals[concatenated_receiving_observatories == observatory][1::2],\n",
    "        bins=nbins,\n",
    "        alpha=transparency,\n",
    "        label=\"Declination\",\n",
    "    )\n",
    "\n",
    "    axs[idx].grid()\n",
    "    axs[idx].set_title(name)\n",
    "    axs[idx].set_ylabel(\"Number of Observations\")\n",
    "    axs[idx].set_xlabel(\"Observation Residual [rad]\")\n",
    "\n",
    "axs[0].legend()\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"Final residual histograms of the {num_observatories} observatories with the most observations for {target_name}\"\n",
    ")\n",
    "fig.set_tight_layout(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it for this tutorial! The final estimation result is quite close to spice at times, but there is clearly plenty of room for improvement in both the **dynamical model** and the **estimation settings**. Consider for example adding weights and biases on observations and links, as well as improved integrator settings and perturbations. \n",
    "\n",
    "If you wanna get more hands-on experience, consider rerunning the script for some other object by changing the `target_mpc_code` variable and seeing how the results change."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
